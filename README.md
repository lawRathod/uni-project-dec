# Real MIA Attack Implementation

## Overview
This project implements membership inference attacks (MIA) on graph neural networks using a realistic threat model where:
- **Target Model**: Trained on real-world graph data (train split)
- **Shadow Model**: Trained on synthetic data generated by DLGrapher
- **Attack Evaluation**: Performed on real non-training data to measure attack effectiveness

## Available Implementations

### 1. rebMIGraph (Modified Original)
**Location**: `rebMIGraph/TSTS.py`
- **Status**: âœ… Working and fully functional
- **Approach**: Modified version of the original rebMIGraph implementation
- **Key Features**:
  - Comprehensive hyperparameter section for easy configuration
  - Custom subgraph data loading for DLGrapher-generated synthetic data
  - TSTS (Train on Subgraph, Test on Subgraph) setting only
  - Support for 4 GNN models: GCN, GAT, SAGE, SGC
  - Real vs synthetic data pipeline with proper edge index remapping
  - Complete attack training and evaluation pipeline

### 2. Realistic MIA Attack (From Scratch)
**Location**: `mia_implementation/attack_impl/realistic_mia_attack.py`
- **Status**: ðŸš§ In development
- **Approach**: Clean implementation written from scratch, inspired by rebMIGraph
- **Target Features**:
  - Modern PyTorch Geometric integration
  - Modular design with clear separation of concerns
  - Enhanced evaluation metrics and statistical analysis
  - Support for cross-model and cross-dataset evaluation

## Datasets and Classification Tasks
The attack will be performed on two datasets with the following classification targets:

### 1. Twitch Dataset
- **Graph type**: Social network of Twitch streamers
- **Node features**: 8 features (views, mature, life_time, created_at, updated_at, dead_account, language, affiliate)
- **Classification target**: `mature` (binary classification)
  - Predicts whether a streamer produces mature content
  - Balanced classes: ~53% non-mature, ~47% mature
  - Alternative targets: `affiliate` status or `dead_account` status

### 2. Event Dataset  
- **Graph type**: Event-based social network
- **Node features**: 5 features (locale, birthyear, gender, joinedAt, timezone)
- **Classification target**: `gender` (binary classification)
  - Predicts user gender (male/female)
  - Balanced classes: ~52% female, ~48% male
  - Alternative target: `locale` for 3-class classification

## Data Preparation
  1. The `rebMIGraph` source uses `torch_geometric.data.Data` objects from `torch_geometric.datasets`.
  2. Our dataset files need to be converted:
     - Real data: `*_train.pt` and `*_nontrain.pt` files contain real subgraphs
     - Synthetic data: `*_synth.pickle` files contain DLGrapher-generated graphs
     - Convert pandas DataFrames and NetworkX graphs to `torch_geometric.data.Data` format
     - Ensure node features, edge indices, and labels are properly formatted
  3. Data allocation:
     - **Target Model**: Uses real `*_train.pt` data
     - **Shadow Model**: Uses synthetic `*_synth.pickle` data
     - **Attack Testing**: Uses real `*_nontrain.pt` data

## Real MIA Attack Pipeline
1. **Dataset Loading & Preparation**
   - Load real training data (`*_train.pt`) for the target model
   - Load synthetic data (`*_synth.pickle`) for the shadow model
   - Load real non-training data (`*_nontrain.pt`) for attack evaluation
   - Convert all data to `torch_geometric.data.Data` format
   - Ensure consistent node features, edge indices, and labels across datasets

2. **Model Training Phase**
   - **Target Model**: 
     - Train on real-world training data (`*_train.pt`)
     - This simulates the victim's model trained on actual data
   - **Shadow Model**: 
     - Train on synthetic data (`*_synth.pickle`) 
     - This simulates the attacker's knowledge using DLGrapher-generated data
   - Both use same GNN architecture (GCN, SAGE, SGC, or GAT)

3. **Posterior Extraction**
   - For Target Model (on real data):
     - **Target In-Train**: Posteriors from real training nodes
     - **Target Out-Train**: Posteriors from real non-training nodes
   - For Shadow Model (on synthetic data):
     - **Shadow In-Train**: Posteriors from synthetic training nodes
     - **Shadow Out-Train**: Posteriors from synthetic test nodes

4. **Attack Model Training**
   - Train attack model using shadow model's posteriors:
     - **Positive samples**: Shadow In-Train posteriors (members)
     - **Negative samples**: Shadow Out-Train posteriors (non-members)
   - Attack model learns to distinguish members from non-members

5. **Attack Evaluation**
   - Test attack model on target model's posteriors:
     - Predict membership for Target In-Train (should predict "member")
     - Predict membership for Target Out-Train (should predict "non-member")
   - This measures how well synthetic-trained attack transfers to real data

6. **Metrics & Analysis**
   - Compute attack performance metrics:
     - Accuracy: Overall correctness of membership predictions
     - AUROC: Area under ROC curve for membership classification
     - Precision/Recall/F1: Detailed classification metrics
   - Compare attack effectiveness across:
     - Different GNN architectures (GCN, GAT, SAGE, SGC)
     - Different datasets (Twitch vs Event)

## Attack Setting: TSTS Only
This implementation focuses exclusively on the **TSTS (Train on Subgraph, Test on Subgraph)** setting because:
- DLGrapher generates collections of synthetic subgraphs (256 samples of 120 nodes each)
- We don't have a single large synthetic graph required for TSTF (Train on Subgraph, Test on Full graph)
- TSTS is more practical for our synthetic data format and aligns with how DLGrapher generates samples

## Key Modifications Needed
1. **Data Loading**: Modify to load different data sources for target/shadow models
2. **Training Pipeline**: Separate data paths for target (real) and shadow (synthetic)
3. **Evaluation**: Ensure attack is tested on real non-training data
4. **TSTS Implementation**: Adapt to handle multiple subgraphs from our dataset format

## Expected Outcomes
- Measure privacy leakage when attackers have access to synthetic data
- Evaluate how well DLGrapher-generated graphs approximate real data for attacks
- Identify which GNN architectures are more vulnerable to MIA

## Comprehensive Implementation Plan

### Phase 1: Data Conversion Pipeline
**Critical Components Needed:**
1. **Custom Data Loader** (`data_converter.py`):
   ```python
   def convert_to_torch_geometric(df, graph, classification_target):
       # Convert pandas DataFrame + NetworkX â†’ torch_geometric.Data
       # Handle feature preprocessing (boolâ†’int, datetimeâ†’timestamp, categoryâ†’encode)
       # Extract labels for classification targets (mature/gender)
       # Convert edge_index from NetworkX to torch tensor format
   ```

2. **Feature Preprocessing** (`feature_processor.py`):
   - Twitch features: views (normalize), mature (0/1), datetime (timestamp), language (encode)
   - Event features: locale (encode), birthyear (normalize), gender (encode), timezone (normalize)
   - Handle missing values and outliers
   - Ensure real/synthetic feature compatibility

### Phase 2: rebMIGraph Modifications  
**Critical Changes Required:**
1. **Data Loading Replacement** (modify TSTS.py lines 83-160):
   - Replace Planetoid/Reddit dataset loading with our custom loader
   - Separate data paths: target uses `*_train.pt`, shadow uses `*_synth.pickle`
   - Evaluation uses `*_nontrain.pt` for real-world testing

2. **Training Pipeline Adaptation** (modify lines 724-800):
   - Update `get_inductive_split()` to handle list of subgraphs instead of single graph
   - Modify training loops to iterate over multiple subgraphs
   - Ensure target and shadow models use different data sources

3. **Classification Target Integration**:
   - Replace hardcoded class numbers with binary classification (2 classes)
   - Update loss functions and metrics for our specific targets

### Phase 3: Evaluation Framework
**Comprehensive Metrics:**
- **Attack Effectiveness**: Accuracy >60%, AUROC >0.7, Precision/Recall/F1
- **Privacy Metrics**: Advantage over random, FPR at 5%, TPR at 1%  
- **Model Utility**: Original task performance, real-synthetic performance gap
- **Statistical Validation**: 10 runs, 95% CI, significance testing

**Experimental Design:**
- 8 main experiments: 2 datasets Ã— 4 GNN architectures
- Ablation studies: data size (64/128/256 samples), feature importance, architecture variants
- Baselines: random, simple ML, ideal attack, no-graph attack
- Transferability: cross-model and cross-dataset evaluation

### Phase 4: Critical Risk Mitigation
**Identified Risks & Solutions:**
1. **Feature Distribution Mismatch**:
   - Risk: Synthetic features differ from real data distributions
   - Solution: Feature normalization, distribution matching, KL divergence validation

2. **Graph Structure Differences**:
   - Risk: Synthetic graphs have different structural properties
   - Solution: Graph metric analysis (degree, clustering, path length), DLGrapher tuning

3. **Insufficient Data Coverage**:
   - Risk: 256 synthetic samples may not capture real data diversity
   - Solution: Generate additional synthetic data, data augmentation techniques

4. **Pandas Version Compatibility**:
   - Risk: Cannot load real data due to version conflicts
   - Solution: Create version-agnostic data loading utilities

### Phase 5: Implementation Priority
**Week 1**: Data conversion pipeline and feature preprocessing
**Week 2**: rebMIGraph modifications and integration testing
**Week 3**: Evaluation framework implementation and validation
**Week 4**: Comprehensive experiments and result analysis

### Code Structure
```
project/
â”œâ”€â”€ data_conversion/
â”‚   â”œâ”€â”€ data_converter.py     # Main conversion logic
â”‚   â”œâ”€â”€ feature_processor.py  # Feature preprocessing
â”‚   â””â”€â”€ validation.py         # Data quality checks
â”œâ”€â”€ attack_implementation/
â”‚   â”œâ”€â”€ modified_TSTS.py      # Updated attack implementation
â”‚   â”œâ”€â”€ custom_models.py      # GNN model definitions
â”‚   â””â”€â”€ training_utils.py     # Training and evaluation utilities
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py            # All evaluation metrics
â”‚   â”œâ”€â”€ experiments.py        # Experimental setup
â”‚   â””â”€â”€ visualization.py      # Result plotting
â””â”€â”€ analysis/
    â”œâ”€â”€ statistical_tests.py  # Significance testing
    â””â”€â”€ privacy_analysis.py   # Privacy implication analysis
```

### Success Criteria
- Successfully convert all dataset formats to torch_geometric
- Achieve >60% attack accuracy on at least one dataset-model combination
- Demonstrate statistical significance over random baseline
- Complete comprehensive ablation studies and transferability analysis
- Validate that synthetic data provides meaningful privacy insights
