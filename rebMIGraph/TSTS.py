# -*- coding: utf-8 -*-
"""Membership Leakage Graph TSTS.ipynb

Automatically generated by Colaboratory.

"""

'''
# This is using different subgraphs for both train and test. Thus TSTS
'''

import os.path as osp
import sys
import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid, Reddit, Flickr
from torch_geometric.nn import GCNConv, SAGEConv, SGConv, GATConv
from tqdm import tqdm
import networkx as nx
import itertools

import time
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
import torch.nn as nn
from torch_geometric.data import NeighborSampler

from torch_geometric.utils import subgraph
from torch_geometric.data import Data
import random

from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, roc_curve, roc_auc_score, f1_score

# Import custom dataset adapter
try:
    from rebmi_adapter import create_inductive_split_custom
    CUSTOM_DATASETS_AVAILABLE = True
except ImportError:
    CUSTOM_DATASETS_AVAILABLE = False
    print("Warning: rebmi_adapter not available. Custom datasets (Twitch/Event) will not work.")

# System configuration
CUDA_DEVICE = 2  # GPU device number

if torch.cuda.is_available():
    torch.cuda.set_device(CUDA_DEVICE)  # change this cos sometimes port 0 is full

num_of_runs = 2#11  # this runs the program 10 times

for which_run in range(1, num_of_runs):
    random_data = os.urandom(4)

    rand_state = int.from_bytes(random_data, byteorder="big") #10, #3469326556, 959554842, 1521097589 #
    print("rand_state", rand_state)
    torch.manual_seed(rand_state)
    random.seed(rand_state)
    np.random.seed(seed=rand_state)

    start_time = time.time()

    # we run each data_type e.g cora against all model type

    '''
    ================================== HYPERPARAMETERS ==================================
    All hyperparameters are centralized here for easy tuning
    '''
    
    # =========================== MODEL AND DATA CONFIGURATION ===========================
    model_type = "GAT"  # GCN, GAT, SAGE, SGC
    data_type = "Custom_Event"  # CiteSeer, Cora, PubMed, Flickr, Reddit, Custom_Twitch, Custom_Event
    mode = "TSTS"  # train on subgraph, test on subgraph
    
    # Custom dataset subset configuration for performance improvement
    # Choose ONE of the following methods:
    # Method 1: Use ratio-based subset (recommended for consistency across datasets)
    CUSTOM_SUBSET_RATIO = 0.2  # Use 30% of subgraphs (0.1=10%, 0.5=50%, 1.0=100%/full data)
    # Method 2: Use explicit maximum number (overrides ratio if set)
    CUSTOM_MAX_SUBGRAPHS = None  # e.g., 10 to use max 10 subgraphs per split, None to use ratio
    
    # Feature engineering configuration
    CUSTOM_ADVANCED_FEATURES = True  # Use advanced graph features (degree, clustering, centrality, etc.)
    
    # =========================== DATASET SPLIT SIZES ===========================
    # Reddit dataset splits
    REDDIT_NUM_TRAIN_PER_CLASS = 500  # 1000
    REDDIT_NUM_TEST_TARGET = 20500  # 41000
    REDDIT_NUM_TEST_SHADOW = 20500  # 41000
    
    # Flickr dataset splits  
    FLICKR_NUM_TRAIN_PER_CLASS = 1500  # min of all classes only have 3k nodes
    FLICKR_NUM_TEST_TARGET = 10500
    FLICKR_NUM_TEST_SHADOW = 10500
    
    # Cora dataset splits
    CORA_NUM_TRAIN_PER_CLASS = 90  # 180
    CORA_NUM_TEST_TARGET = 630
    CORA_NUM_TEST_SHADOW = 630
    
    # CiteSeer dataset splits
    CITESEER_NUM_TRAIN_PER_CLASS = 100
    CITESEER_NUM_TEST_TARGET = 600  
    CITESEER_NUM_TEST_SHADOW = 600
    
    # PubMed dataset splits
    PUBMED_NUM_TRAIN_PER_CLASS = 1500
    PUBMED_NUM_TEST_TARGET = 4500
    PUBMED_NUM_TEST_SHADOW = 4500
    
    # Custom datasets (Twitch, Event) - handled by rebmi_adapter
    # No per-class splits needed as adapter handles subgraph combination
    
    # =========================== MODEL ARCHITECTURE ===========================
    # GAT hyperparameters
    GAT_HIDDEN_DIM = 8
    GAT_HEADS = 8
    GAT_DROPOUT = 0.1
    
    # NeighborSampler hyperparameters
    NEIGHBOR_SAMPLE_SIZES = [25, 10]  # sampling sizes for 2-hop neighborhood
    BATCH_SIZE_NEIGHBOR = 64
    
    # =========================== TRAINING HYPERPARAMETERS ===========================
    # Learning rates
    LR_SAGE_PUBMED = 0.0001  # SAGE model on PubMed
    LR_SAGE_OTHER = 0.001    # SAGE model on other datasets
    LR_DEFAULT = 0.0001      # Default learning rate for GCN, GAT, SGC
    LR_CUSTOM = 0.01         # Higher LR for custom datasets with few features
    
    # Attack model hyperparameters (optimized for better performance)
    USE_IMPROVED_ATTACK = True  # Toggle to use improved attack model and settings
    
    if USE_IMPROVED_ATTACK:
        # Optimized hyperparameters for better attack performance
        ATTACK_LR = 0.0005       # Even lower LR for more stable training
        ATTACK_EPOCHS = 150      # More epochs for better convergence
        ATTACK_BATCH_SIZE = 64   # Larger batch for stability
        ATTACK_TEST_BATCH_SIZE = 128
        ATTACK_DROPOUT = 0.3     # Dropout to prevent overfitting
        ATTACK_WEIGHT_DECAY = 1e-4  # L2 regularization
        TEMPERATURE = 1.5        # Temperature scaling for calibrated posteriors (reduced from 2.0)
    else:
        # Original hyperparameters
        ATTACK_LR = 0.01         
        ATTACK_EPOCHS = 100      
        ATTACK_BATCH_SIZE = 32
        ATTACK_TEST_BATCH_SIZE = 64
        ATTACK_DROPOUT = 0.0
        ATTACK_WEIGHT_DECAY = 0
        TEMPERATURE = 1.0        # No temperature scaling
    
    # Model training epochs
    SAGE_EPOCHS_CORA_CITESEER = 16   # 301 #16 for CiteSeer n Cora
    SAGE_EPOCHS_PUBMED = 101         # 101 for PubMed
    SAGE_EPOCHS_FLICKR_REDDIT = 301  # 301 for Flickr n Reddit
    DEFAULT_EPOCHS = 301             # Default for non-SAGE models
    

    save_shadow_OutTrain = "posteriorsShadowOut_" + mode + "_" + data_type + "_" + model_type + ".txt"
    save_shadow_InTrain = "posteriorsShadowTrain_" + mode + "_" + data_type + "_" + model_type + ".txt"
    save_target_OutTrain = "posteriorsTargetOut_" + mode + "_" + data_type + "_" + model_type + ".txt"
    save_target_InTrain = "posteriorsTargetTrain_" + mode + "_" + data_type + "_" + model_type + ".txt"

    save_target_InTrain_nodes_neigbors = "nodesNeigborsTargetTrain_" + mode + "_" + data_type + "_" + model_type + ".npy"
    save_target_OutTrain_nodes_neigbors = "nodesNeigborsTargetOut_" + mode + "_" + data_type + "_" + model_type + ".npy"

    result_file = open("resultfile_" + mode + "_" + model_type + ".txt", "a")

    '''
    ######################################## Data ##############################################
    '''
    custom_dataset = False  # Flag for custom datasets
    
    if data_type == "Reddit":
        ###################################### Reddit ##################################

        # path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Reddit')
        path = "./data/Reddit"
        dataset = Reddit(path)
        # data = dataset[0]
        # print("len(dataset)", len(dataset)) # Reddit dataset consists of 1 graph
        # print("data", data) # Data(edge_index=[2, 114615892], test_mask=[232965], train_mask=[232965], val_mask=[232965], x=[232965, 602], y=[232965])
        # print("Total Num of nodes in dataset", data.num_nodes) # 232965
        # print("Total Num of edges in dataset", data.num_edges) # 114615892
        # print("Total Num of node features in dataset", data.num_node_features) # 602
        # print("Total Num of features in dataset", dataset.num_features) # same as node features # 602
        # print("Num classes", dataset.num_classes) #41

        # Reduced this cos it's taking too long to create subgraph
        num_train_Train_per_class = REDDIT_NUM_TRAIN_PER_CLASS
        num_train_Shadow_per_class = REDDIT_NUM_TRAIN_PER_CLASS
        num_test_Target = REDDIT_NUM_TEST_TARGET
        num_test_Shadow = REDDIT_NUM_TEST_SHADOW

    elif data_type == "Flickr":

        ###################################### Flickr ##################################

        path = "./data/Flickr"
        dataset = Flickr(path)
        data = dataset[0]
        # print("len(dataset)", len(dataset))  # Flikr dataset consists of 1 graph
        # print("data",
        #       data)  # Data(edge_index=[2, 899756], test_mask=[89250], train_mask=[89250], val_mask=[89250], x=[89250, 500], y=[89250])
        # print("Total Num of nodes in dataset", data.num_nodes)  # 89250
        # print("Total Num of edges in dataset", data.num_edges)  # 899756
        # print("Total Num of node features in dataset", data.num_node_features)  # 500
        # print("Total Num of features in dataset", dataset.num_features)  # same as node features # 500
        # print("Num classes", dataset.num_classes)  # 7

        num_train_Train_per_class = FLICKR_NUM_TRAIN_PER_CLASS  # cos min of all classes only have 3k nodes
        num_train_Shadow_per_class = FLICKR_NUM_TRAIN_PER_CLASS
        num_test_Target = FLICKR_NUM_TEST_TARGET
        num_test_Shadow = FLICKR_NUM_TEST_SHADOW

    elif data_type == "Cora":

        ###################################### Cora ##################################

        dataset = Planetoid(root='./data/Cora', name='Cora', split="random")  # set test to 1320 to match train
        num_train_Train_per_class = CORA_NUM_TRAIN_PER_CLASS  # 180
        num_train_Shadow_per_class = CORA_NUM_TRAIN_PER_CLASS
        num_test_Target = CORA_NUM_TEST_TARGET
        num_test_Shadow = CORA_NUM_TEST_SHADOW

    elif data_type == "CiteSeer":

        ###################################### CiteSeer ##################################

        dataset = Planetoid(root='./data/CiteSeer', name='CiteSeer', split="random")
        num_train_Train_per_class = CITESEER_NUM_TRAIN_PER_CLASS
        num_train_Shadow_per_class = CITESEER_NUM_TRAIN_PER_CLASS
        num_test_Target = CITESEER_NUM_TEST_TARGET
        num_test_Shadow = CITESEER_NUM_TEST_SHADOW


    elif data_type == "PubMed":

        ###################################### Reddit ##################################

        dataset = Planetoid(root='./data/PubMed', name='PubMed', split="random")

        num_train_Train_per_class = PUBMED_NUM_TRAIN_PER_CLASS
        num_train_Shadow_per_class = PUBMED_NUM_TRAIN_PER_CLASS
        num_test_Target = PUBMED_NUM_TEST_TARGET
        num_test_Shadow = PUBMED_NUM_TEST_SHADOW

    elif data_type.startswith("Custom_"):
        ###################################### Custom Datasets (Twitch, Event) ##################################
        
        if not CUSTOM_DATASETS_AVAILABLE:
            print("Error: Custom datasets require rebmi_adapter.py")
            exit(1)
            
        # Extract dataset name (Custom_Twitch -> twitch)
        dataset_name = data_type.split("_", 1)[1].lower()
        print(f"Loading custom dataset: {dataset_name}")
        
        # Create inductive split with subset configuration (returns data + metadata)
        data_new, num_features, num_classes = create_inductive_split_custom(dataset_name, subset_ratio=CUSTOM_SUBSET_RATIO, max_subgraphs=CUSTOM_MAX_SUBGRAPHS, advanced_features=CUSTOM_ADVANCED_FEATURES)
        
        print(f"Custom dataset loaded: {num_features} features, {num_classes} classes")
        
        # Create a mock dataset object for compatibility
        class MockDataset:
            def __init__(self, num_features, num_classes):
                self.num_features = num_features
                self.num_node_features = num_features  # For compatibility
                self.num_classes = num_classes
        
        dataset = MockDataset(num_features, num_classes)
        
        # Set variables needed for NeighborSampler (use actual node counts from each split)
        num_train_Target = data_new.target_x.shape[0] if data_new.target_x is not None else 0
        num_test_Target = data_new.target_test_x.shape[0] if data_new.target_test_x is not None else 0
        num_train_Shadow = data_new.shadow_x.shape[0] if data_new.shadow_x is not None else 0
        num_test_Shadow = data_new.shadow_test_x.shape[0] if data_new.shadow_test_x is not None else 0
        
        # Validate edge indices don't exceed node counts
        if data_new.target_edge_index is not None and data_new.target_edge_index.numel() > 0:
            max_node_idx = data_new.target_edge_index.max().item()
            if max_node_idx >= num_train_Target:
                print(f"Warning: Target train edge_index max ({max_node_idx}) >= num_nodes ({num_train_Target})")
        
        print(f"Custom dataset node counts - Train: Target={num_train_Target}, Shadow={num_train_Shadow}, Test: Target={num_test_Target}, Shadow={num_test_Shadow}")
        
        # Set len_y for custom datasets (total number of nodes in target dataset)
        len_y = num_train_Target + num_test_Target
        
        # Skip the normal dataset loading and inductive split creation
        custom_dataset = True

    else:
        print("Error: No data specified")

    # Skip normal dataset processing for custom datasets
    if not custom_dataset:
        data = dataset[0]
        len_y = data.y.size(0)
        print("data", data)





    '''
    ############################################# Target and Shadow Models ###############################################
    '''


    class TargetModel(torch.nn.Module):
        def __init__(self, dataset):
            super(TargetModel, self).__init__()

            if model_type == "GCN":
                # GCN - enhanced for custom datasets
                if data_type.startswith("Custom_"):
                    # Smaller model to encourage overfitting for better MIA signal
                    self.conv1 = GCNConv(dataset.num_node_features, 32)
                    self.conv2 = GCNConv(32, dataset.num_classes)
                    self.conv3 = None
                    self.dropout = 0.2
                else:
                    self.conv1 = GCNConv(dataset.num_node_features, 256)
                    self.conv2 = GCNConv(256, dataset.num_classes)
                    self.conv3 = None
                    self.dropout = 0.0
            elif model_type == "SAGE":
                # GraphSage
                # self.conv1 = SAGEConv(dataset.num_node_features, 256)
                # self.conv2 = SAGEConv(256, dataset.num_classes)

                self.num_layers = 2

                self.convs = torch.nn.ModuleList()
                self.convs.append(SAGEConv(dataset.num_node_features, 256))
                self.convs.append(SAGEConv(256, dataset.num_classes))
                self.conv3 = None  # Not used for SAGE
                self.dropout = 0.0  # Set dropout for SAGE

            elif model_type == "SGC":
                # SGC
                self.conv1 = SGConv(dataset.num_node_features, 256, K=2, cached=False)
                self.conv2 = SGConv(256, dataset.num_classes, K=2, cached=False)
                self.conv3 = None  # Not used for SGC
                self.dropout = 0.0  # Set dropout for SGC

            elif model_type == "GAT":
                # GAT
                self.conv1 = GATConv(dataset.num_features, GAT_HIDDEN_DIM, heads=GAT_HEADS, dropout=GAT_DROPOUT)
                # On the Pubmed dataset, use heads=8 in conv2.
                if data_type == "PubMed":
                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False)
                    # self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False, dropout=0.1)
                else:
                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=False)
                self.conv3 = None  # Not used for GAT
                self.dropout = 0.0  # Set dropout for GAT
            else:
                print("Error: No model selected")
                self.conv3 = None  # Default
                self.dropout = 0.0  # Default dropout

        def forward(self, x, edge_index):
            # print("xxxxxxx", x.size())

            if model_type == "SAGE":
                all_node_and_neigbors = []  # {} # lookup table dictionary #changed to list cos it;s not possible to slice esp for TSTS
                all_nodes = []

                # the edge_index here is quite different (it is a list cos we will be passing train_loader). edge index is different based on batch data.
                # Note edge_index here is a bipartite graph. meaning all the edges retured are connected
                for i, (edge_ind, _, size) in enumerate(edge_index):
                    # print("iiiiiiiiiiiiiiii", i)
                    # print("edge_ind", edge_ind)

                    edges_raw = edge_ind.cpu().numpy()
                    # print("edges_raw", edges_raw)
                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]

                    G = nx.Graph()
                    G.add_nodes_from(list(range(
                        num_test_Target)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class
                    G.add_edges_from(edges)

                    # print("x.size(0) forward", x.size(0))
                    # print("x.size forward", x.size())
                    # getting the neigbors of a particular node.
                    for n in range(0, x.size(
                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!
                        all_nodes.append(n)  # get all nodes
                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list
                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))
                        # print("n:", n, "neighbors:", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n

                    # print("all_node_and_neigbors", all_node_and_neigbors)

                    x_target = x[:size[1]]  # Target nodes are always placed first.
                    x = self.convs[i]((x, x_target), edge_ind)
                    if i != self.num_layers - 1:
                        x = F.relu(x)
                        # x = F.dropout(x, p=0.5, training=self.training)

                # print("Final all nodes and neighbors", all_node_and_neigbors)
                return x.log_softmax(dim=-1), all_node_and_neigbors

            else:
                # torch.set_printoptions(threshold=10000)
                # print("x0", x[0])
                # print("x1", x[1])
                # print("x", x.shape)
                # print("edge_index", edge_index.shape)

                # Begin edit data for passing node n it's neghbprs:
                edges_raw = edge_index.cpu().numpy()
                # print("edges_raw", edges_raw)
                edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]

                G = nx.Graph()
                G.add_nodes_from(list(range(
                    num_test_Target)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class
                G.add_edges_from(edges)

                all_node_and_neigbors = []
                all_nodes = []

                # print("x.size(0) shadow forward", x.size(0))
                # getting the neigbors of a particular node.
                for n in range(0, x.size(
                        0)):  # x.size(0) caters for when u input the full graph for for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!
                    all_nodes.append(n)  # get all nodes
                    # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list
                    all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))
                    # print("n:", n, "neighbors:", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n
                # print("all_node_and_neigbors", all_node_and_neigbors)

                x, edge_index = x, edge_index
                x = F.relu(self.conv1(x, edge_index))
                x = F.dropout(x, p=self.dropout, training=self.training)
                if self.conv3 is not None:
                    x = F.relu(self.conv2(x, edge_index))
                    x = F.dropout(x, p=self.dropout, training=self.training)
                    x = self.conv3(x, edge_index)
                else:
                    x = self.conv2(x, edge_index)
                # x = F.relu(x)
                # x = F.normalize(x, p=2, dim=-1)

                return F.log_softmax(x, dim=1), all_node_and_neigbors



    class ShadowModel(torch.nn.Module):
        def __init__(self, dataset):
            super(ShadowModel, self).__init__()

            if model_type == "GCN":
                # GCN - enhanced for custom datasets
                if data_type.startswith("Custom_"):
                    # Smaller model to encourage overfitting for better MIA signal
                    self.conv1 = GCNConv(dataset.num_node_features, 32)
                    self.conv2 = GCNConv(32, dataset.num_classes)
                    self.conv3 = None
                    self.dropout = 0.2
                else:
                    self.conv1 = GCNConv(dataset.num_node_features, 256)
                    self.conv2 = GCNConv(256, dataset.num_classes)
                    self.conv3 = None
                    self.dropout = 0.0
            elif model_type == "SAGE":
                # GraphSage
                # self.conv1 = SAGEConv(dataset.num_node_features, 256)
                # self.conv2 = SAGEConv(256, dataset.num_classes)

                self.num_layers = 2

                self.convs = torch.nn.ModuleList()
                self.convs.append(SAGEConv(dataset.num_node_features, 256))
                self.convs.append(SAGEConv(256, dataset.num_classes))
                self.conv3 = None  # Not used for SAGE
                self.dropout = 0.0  # Set dropout for SAGE

            elif model_type == "SGC":
                # SGC
                self.conv1 = SGConv(dataset.num_node_features, 256, K=2, cached=False)
                self.conv2 = SGConv(256, dataset.num_classes, K=2, cached=False)
                self.conv3 = None  # Not used for SGC
                self.dropout = 0.0  # Set dropout for SGC

            elif model_type == "GAT":
                # GAT
                self.conv1 = GATConv(dataset.num_features, GAT_HIDDEN_DIM, heads=GAT_HEADS, dropout=GAT_DROPOUT)
                # On the Pubmed dataset, use heads=8 in conv2.
                if data_type == "PubMed":
                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False)
                    # self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False, dropout=0.1)
                else:
                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=False)
                self.conv3 = None  # Not used for GAT
                self.dropout = 0.0  # Set dropout for GAT
            else:
                print("Error: No model selected")
                self.conv3 = None  # Default
                self.dropout = 0.0  # Default dropout

        def forward(self, x, edge_index):
            # print("xxxxxxx", x.size())
            if model_type == "SAGE":
                all_node_and_neigbors = []
                all_nodes = []

                # the edge_index here is quite different (it is a list cos we will be passing train_loader). edge index is different based on batch data.
                # Note edge_index here is a bipartite graph. meaning all the edges retured are connected
                for i, (edge_ind, _, size) in enumerate(edge_index):
                    # print("iiiiiiiiiiiiiiii", i)
                    edges_raw = edge_ind.cpu().numpy()
                    # print("edges_raw", edges_raw)
                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]

                    G = nx.Graph()
                    G.add_nodes_from(list(range(
                        num_test_Target)))  # this is set to num_test_Shadow cos that's the max no of nodes since num_test_Shadow = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Shadow
                    G.add_edges_from(edges)

                    # print("x.size(0) shadow forward", x.size(0))
                    # print("x.size", x.size())
                    # getting the neigbors of a particular node
                    for n in range(0, x.size(
                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!
                        all_nodes.append(n)  # get all nodes
                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list
                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))
                        # print("n:", n, "neighbors:", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n

                    # print("all_node_and_neigbors", all_node_and_neigbors)

                    x_shadow = x[:size[1]]  # Shadow nodes are always placed first.
                    x = self.convs[i]((x, x_shadow), edge_ind)
                    if i != self.num_layers - 1:
                        x = F.relu(x)
                        # x = F.dropout(x, p=0.5, training=self.training)

                # print("Final all nodes and neighbors", all_node_and_neigbors)
                return x.log_softmax(dim=-1), all_node_and_neigbors

            else:
                edges_raw = edge_index.cpu().numpy()
                # print("edges_raw", edges_raw)
                edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]

                G = nx.Graph()
                G.add_nodes_from(list(range(
                    num_test_Target)))  # this is set to num_test_Shadow cos that's the max no of nodes since num_test_Shadow = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Shadow
                G.add_edges_from(edges)

                all_node_and_neigbors = []
                all_nodes = []

                # print("x.size(0)", x.size(0))
                # getting the neigbors of a particular node
                for n in range(0, x.size(
                        0)):  # x.size(0) caters for when u input the full graph for for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!
                    all_nodes.append(n)  # get all nodes
                    # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list
                    all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))
                    # print("n:", n, "neighbors:", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n
                # print("all_node_and_neigbors", all_node_and_neigbors)



                x, edge_index = x, edge_index
                x = F.relu(self.conv1(x, edge_index))
                x = F.dropout(x, p=self.dropout, training=self.training)
                if self.conv3 is not None:
                    x = F.relu(self.conv2(x, edge_index))
                    x = F.dropout(x, p=self.dropout, training=self.training)
                    x = self.conv3(x, edge_index)
                else:
                    x = self.conv2(x, edge_index)
                # x = F.relu(x)
                # x = F.normalize(x, p=2, dim=-1)

                return F.log_softmax(x, dim=1), all_node_and_neigbors






    '''
    ##################################### Data Processing Inductive Split ######################################
    '''


    def get_inductive_spilt(data, num_classes, num_train_Train_per_class, num_train_Shadow_per_class, num_test_Target,
                            num_test_Shadow):
        # -----------------------------------------------------------------------
        # target_train, target_out
        # shadow_train, shadow_out
        '''
        Randomly choose 'num_train_Train_per_class' and 'num_train_Shadow_per_class' per classes for training Target and shadow models respectively
        Random choose 'num_test_Target' and 'num_test_Shadow' for testing (out data) Target and shadow models respectively

        '''

        # convert all label to list
        label_idx = data.y.numpy().tolist()
        print("label_idx", len(label_idx))
        target_train_idx = []
        shadow_train_idx = []
        # for i in range(num_classes):
        #     c = [x for x in range(len(label_idx)) if label_idx[x] == i]
        #     print("c", len(c)) #the min is 180 which is 7th class
        #     sample = random.sample(range(c),num_train_Train_per_class)
        #     target_train_idx.extend(sample)

        for c in range(num_classes):
            idx = (data.y == c).nonzero().view(-1)
            sample_train_idx = idx[torch.randperm(idx.size(0))]
            sample_target_train_idx = sample_train_idx[:num_train_Train_per_class]
            target_train_idx.extend(sample_target_train_idx)

            print("idx.size(0)", idx.size(0))  # this is the total number of data in each class

            # Ensure they don't over lap
            # sample_shadow_train_idx = idx[torch.randperm(idx.size(0))[num_train_Train_per_class:num_train_Train_per_class + num_train_Shadow_per_class]]
            sample_shadow_train_idx = sample_train_idx[
                                      num_train_Train_per_class:num_train_Train_per_class + num_train_Shadow_per_class]
            shadow_train_idx.extend(sample_shadow_train_idx)

        print("shadow_train_idx", len(shadow_train_idx))
        print("Target_train_idx", len(target_train_idx))

        others = [x for x in range(len(label_idx)) if x not in set(target_train_idx) and x not in set(shadow_train_idx)]
        # print("others",others)
        print("done others")
        target_test_idx = random.sample(others, num_test_Target)
        print("done target test")
        shadow_test = [x for x in others if x not in set(target_test_idx)]
        shadow_test_idx = random.sample(shadow_test, num_test_Shadow)
        print("done shadow test")

        print("target_test_idx", len(target_test_idx))
        print("shadow_test_idx", len(shadow_test_idx))

        # ----------set values for mask--------------------------------
        # Also changed this so as to conform with the shape of others. No, this uncommented one is better
        # We don't need this cos we have already created a subgraph outta it

        target_train_mask = torch.ones(len(target_train_idx), dtype=torch.uint8)
        shadow_train_mask = torch.ones(len(shadow_train_idx), dtype=torch.uint8)

        # target_train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)
        # for i in target_train_idx:
        #     target_train_mask[i] = 1

        # ---test-mask---
        target_test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)
        for i in target_test_idx:
            target_test_mask[i] = 1
        # ---val-mask-----
        shadow_test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)
        for i in shadow_test_idx:
            shadow_test_mask[i] = 1

        '''
        get all nodes and corresponding edge_index information
        '''
        # This is for creating subgraphs

        # For target

        # train
        target_x_inductive = data.x[target_train_idx]
        target_y_inductive = data.y[target_train_idx]
        target_edge_index_inductive, _ = subgraph(target_train_idx, data.edge_index)

        # test
        target_x_test_inductive = data.x[target_test_idx]
        target_y_test_inductive = data.y[target_test_idx]
        target_test_edge_index_inductive, _ = subgraph(target_test_idx, data.edge_index)

        # For shadow
        shadow_x_inductive = data.x[shadow_train_idx]
        shadow_y_inductive = data.y[shadow_train_idx]
        shadow_edge_index_inductive, _ = subgraph(shadow_train_idx, data.edge_index)

        # test
        shadow_x_test_inductive = data.x[shadow_test_idx]
        shadow_y_test_inductive = data.y[shadow_test_idx]
        shadow_test_edge_index_inductive, _ = subgraph(shadow_test_idx, data.edge_index)

        '''
        in this part use a vertex_map to get a correct target_edge_index_inductive
        '''
        # ---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*
        '''
        get new edge_index information, because some nodes were removed from orginal nodes set. so there
        are some edge_index that will disappear. If we dont do that, it will cause error: out of index 193
        '''

        target_vertex_map = {}
        ind = -1
        for i in range(data.num_nodes):
            if i in target_train_idx:
                ind += 1
                target_vertex_map[i] = ind
        for i in range(target_edge_index_inductive.shape[1]):
            target_edge_index_inductive[0, i] = target_vertex_map[target_edge_index_inductive[0, i].tolist()]
            target_edge_index_inductive[1, i] = target_vertex_map[target_edge_index_inductive[1, i].tolist()]

        target_test_vertex_map = {}
        ind = -1
        for i in range(data.num_nodes):
            if i in target_test_idx:
                ind += 1
                target_test_vertex_map[i] = ind
        for i in range(target_test_edge_index_inductive.shape[1]):
            target_test_edge_index_inductive[0, i] = target_test_vertex_map[
                target_test_edge_index_inductive[0, i].tolist()]
            target_test_edge_index_inductive[1, i] = target_test_vertex_map[
                target_test_edge_index_inductive[1, i].tolist()]

        shadow_vertex_map = {}
        ind = -1
        for i in range(data.num_nodes):
            if i in shadow_train_idx:
                ind += 1
                shadow_vertex_map[i] = ind
        for i in range(shadow_edge_index_inductive.shape[1]):
            shadow_edge_index_inductive[0, i] = shadow_vertex_map[shadow_edge_index_inductive[0, i].tolist()]
            shadow_edge_index_inductive[1, i] = shadow_vertex_map[shadow_edge_index_inductive[1, i].tolist()]

        shadow_test_vertex_map = {}
        ind = -1
        for i in range(data.num_nodes):
            if i in shadow_test_idx:
                ind += 1
                shadow_test_vertex_map[i] = ind
        for i in range(shadow_test_edge_index_inductive.shape[1]):
            shadow_test_edge_index_inductive[0, i] = shadow_test_vertex_map[
                shadow_test_edge_index_inductive[0, i].tolist()]
            shadow_test_edge_index_inductive[1, i] = shadow_test_vertex_map[
                shadow_test_edge_index_inductive[1, i].tolist()]

        # ---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*

        # All graph data
        all_x = data.x
        all_y = data.y
        all_edge_index = data.edge_index

        '''
        now we create a New data instances for save the all data with that we do inductive learning tasks
        '''

        data = Data(target_x=target_x_inductive, target_edge_index=target_edge_index_inductive,
                    target_y=target_y_inductive,
                    target_test_x=target_x_test_inductive, target_test_edge_index=target_test_edge_index_inductive,
                    target_test_y=target_y_test_inductive,
                    shadow_x=shadow_x_inductive, shadow_edge_index=shadow_edge_index_inductive,
                    shadow_y=shadow_y_inductive,
                    shadow_test_x=shadow_x_test_inductive, shadow_test_edge_index=shadow_test_edge_index_inductive,
                    shadow_test_y=shadow_y_test_inductive,
                    target_train_mask=target_train_mask, shadow_train_mask=shadow_train_mask, all_x=all_x,
                    all_edge_index=all_edge_index, all_y=all_y, target_test_mask=target_test_mask,
                    shadow_test_mask=shadow_test_mask)

        # Original
        # data = Data(target_x=target_x_inductive,target_edge_index=target_edge_index_inductive,target_y=target_y_inductive,
        #                 shadow_x=shadow_x_inductive, shadow_edge_index=shadow_edge_index_inductive,shadow_y=shadow_y_inductive,
        #                 target_train_mask=target_train_mask,shadow_train_mask=shadow_train_mask, all_x=all_x,
        #                 all_edge_index=all_edge_index,all_y=all_y, target_test_mask=target_test_mask, shadow_test_mask=shadow_test_mask)

        # # flipping the shadow to target and target to shadow
        # data = Data(target_x=shadow_x_inductive,target_edge_index=shadow_edge_index_inductive,target_y=shadow_y_inductive,
        #                 shadow_x=target_x_inductive, shadow_edge_index=target_edge_index_inductive,shadow_y=target_y_inductive,
        #                 target_train_mask=shadow_train_mask,shadow_train_mask=target_train_mask,
        #                 all_x=all_x,all_edge_index=all_edge_index,all_y=all_y,
        #                 target_test_mask=shadow_test_mask, shadow_test_mask=target_test_mask)

        return data


    def get_train_acc(data_new, pred, isTarget=True):
        if isTarget:
            # Removed train mask cos u r testing on the subgraph only tho
            # train_acc = pred.eq(data_new.target_y[data_new.target_train_mask]).sum().item() / data_new.target_train_mask.sum().item()
            train_acc = pred.eq(data_new.target_y).sum().item() / data_new.target_train_mask.sum().item()
        else:
            # train_acc = pred.eq(data_new.target_y[data_new.shadow_train_mask]).sum().item() / data_new.shadow_train_mask.sum().item()
            train_acc = pred.eq(data_new.shadow_y).sum().item() / data_new.shadow_train_mask.sum().item()
        # I changed this so as to get the prediction when u test on the train dataset cos all_y has all y. No this approach is not good. The apporach above is accurate
        # train_acc = pred.eq(data_new.all_y[data_new.target_train_mask]).sum().item() / data_new.target_train_mask.sum().item()
        return train_acc


    def get_test_acc(data_new, pred, isTarget=True):
        if isTarget:
            # test_acc = pred.eq(data_new.all_y[data_new.target_test_mask]).sum().item() / data_new.target_test_mask.sum().item()
            test_acc = pred.eq(data_new.target_test_y).sum().item() / data_new.target_test_mask.sum().item()
        else:
            # test_acc = pred.eq(
            #     data_new.all_y[data_new.shadow_test_mask]).sum().item() / data_new.shadow_test_mask.sum().item()
            test_acc = pred.eq(data_new.shadow_test_y).sum().item() / data_new.shadow_test_mask.sum().item()

        return test_acc


    def get_marco_f1(data_new, pred_labels, true_labels, label_list):
        # f1_marco = f1_score(true_labels,pred_labels,label_list,average='macro')
        f1_marco = f1_score(true_labels.cpu().numpy(), pred_labels, average='macro')
        return f1_marco


    def get_micro_f1(data_new, pred_labels, true_labels, label_list):
        # f1_micro = f1_score(true_labels,pred_labels,label_list,average='micro')
        f1_micro = f1_score(true_labels.cpu().numpy(), pred_labels, average='micro')
        return f1_micro


    '''
    ########################## End Data Processing Inductive Split ###########################
    '''

    ''' Data initalization '''

    # --- create labels_list---
    label_list = [x for x in range(dataset.num_classes)]

    # Skip normal inductive split for custom datasets
    if not custom_dataset:
        # convert all label to list
        label_idx = data.y.numpy().tolist()

        data_new = get_inductive_spilt(data, dataset.num_classes, num_train_Train_per_class, num_train_Shadow_per_class,
                                       num_test_Target, num_test_Shadow)

    print("data new", data_new)
    print("data_new.shadow_test_mask.sum()", data_new.shadow_test_mask.sum())
    print("data_new.target_test_mask.sum()", data_new.target_test_mask.sum())

    print(dataset.num_classes)

    # For custom datasets, create bool tensors with correct sizes for each split
    if data_type.startswith("Custom_"):
        # Each loader needs its own bool_tensor with the correct size
        target_train_bool = torch.ones(num_train_Target, dtype=torch.bool)
        target_test_bool = torch.ones(num_test_Target, dtype=torch.bool)
        shadow_train_bool = torch.ones(num_train_Shadow, dtype=torch.bool)
        shadow_test_bool = torch.ones(num_test_Shadow, dtype=torch.bool)
        
        target_train_loader = NeighborSampler(data_new.target_edge_index, node_idx=target_train_bool,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_train_Target, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)
        
        target_test_loader = NeighborSampler(data_new.target_test_edge_index, node_idx=target_test_bool,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_test_Target, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)
        
        shadow_train_loader = NeighborSampler(data_new.shadow_edge_index, node_idx=shadow_train_bool,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_train_Shadow, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)
        
        shadow_test_loader = NeighborSampler(data_new.shadow_test_edge_index, node_idx=shadow_test_bool,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_test_Shadow, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)
        
        print(f"Custom dataset loaders initialized:")
        print(f"  Target train: {num_train_Target} nodes, edge_index shape: {data_new.target_edge_index.shape}")
        print(f"  Target test: {num_test_Target} nodes, edge_index shape: {data_new.target_test_edge_index.shape if data_new.target_test_edge_index is not None else 'None'}")
        print(f"  Shadow train: {num_train_Shadow} nodes, edge_index shape: {data_new.shadow_edge_index.shape if data_new.shadow_edge_index is not None else 'None'}")
        print(f"  Shadow test: {num_test_Shadow} nodes, edge_index shape: {data_new.shadow_test_edge_index.shape if data_new.shadow_test_edge_index is not None else 'None'}")
    else:
        # Original code for non-custom datasets
        bool_tensor = torch.ones(num_test_Target, dtype=torch.bool)
        
        target_train_loader = NeighborSampler(data_new.target_edge_index, node_idx=bool_tensor,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_test_Target, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)
        
        target_test_loader = NeighborSampler(data_new.target_test_edge_index, node_idx=bool_tensor,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_test_Target, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)
        
        shadow_train_loader = NeighborSampler(data_new.shadow_edge_index, node_idx=bool_tensor,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_test_Shadow, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)
        
        shadow_test_loader = NeighborSampler(data_new.shadow_test_edge_index, node_idx=bool_tensor,
                                       sizes=NEIGHBOR_SAMPLE_SIZES, num_nodes=num_test_Shadow, batch_size=BATCH_SIZE_NEIGHBOR, shuffle=False)





    ''' Model initialization '''

    target_model = TargetModel(dataset)
    shadow_model = ShadowModel(dataset)  # Defining it as TargetModel(dataset) still produces the same result. I explicitly redefined each model for clarity

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    data_new = data_new.to(device)

    target_model = target_model.to(device)

    # # To reset the model to default
    # for name, module in target_model.named_children():
    #     print('resetting ', name)
    #     module.reset_parameters()

    shadow_model = shadow_model.to(device)

    print("model", target_model)
    if data_type.startswith("Custom_"):
        # Use higher learning rate for custom datasets with few features
        target_optimizer = torch.optim.Adam(target_model.parameters(), lr=LR_CUSTOM, weight_decay=5e-4)
        shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=LR_CUSTOM, weight_decay=5e-4)
    elif model_type == "SAGE":
        # better attack but slighly less test acc
        if data_type == "PubMed":
            target_optimizer = torch.optim.Adam(target_model.parameters(), lr=LR_SAGE_PUBMED)
            shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=LR_SAGE_PUBMED)  # 01
        else:
            target_optimizer = torch.optim.Adam(target_model.parameters(), lr=LR_SAGE_OTHER)
            shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=LR_SAGE_OTHER) #01
    else:
        target_optimizer = torch.optim.Adam(target_model.parameters(), lr=LR_DEFAULT)
        shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=LR_DEFAULT)

    '''
    Train and Test function for model
    '''


    # --*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*
    # #----------------------------- TRAIN FUCNTION---------------------------
    def train(model, optimizer, isTarget=True):
        model.train()
        optimizer.zero_grad()
        if isTarget:
            out, nodes_and_neighbors = model(data_new.target_x, data_new.target_edge_index)
            loss = F.nll_loss(out, data_new.target_y)
        else:
            out, nodes_and_neighbors = model(data_new.shadow_x, data_new.shadow_edge_index)
            loss = F.nll_loss(out, data_new.shadow_y)

        pred = torch.exp(out)
        # print("pred", pred)

        loss.backward()
        optimizer.step()

        # approximate accuracy
        if isTarget:
            train_loss = loss.item() / int(data_new.target_train_mask.sum())
            total_correct = int(pred.argmax(dim=-1).eq(data_new.target_y).sum()) / int(data_new.target_train_mask.sum())
        else:
            train_loss = loss.item() / int(data_new.shadow_train_mask.sum())
            total_correct = int(pred.argmax(dim=-1).eq(data_new.shadow_y).sum()) / int(data_new.shadow_train_mask.sum())

        # print("End train")
        return total_correct, train_loss




    def train_SAGE(model, optimizer, isTarget=True):
        # print("================ Begin SAGE Train ==================")
        model.train()

        if isTarget:
            total_loss = total_correct = 0
            for batch_size, n_id, adjs in target_train_loader:
                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.
                adjs = [adj.to(device) for adj in adjs]
                # print("adjs", adjs)

                optimizer.zero_grad()
                out, nodes_and_neighbors = model(data_new.target_x[n_id], adjs)
                # print("out traaaaain", out.shape)
                loss = F.nll_loss(out, data_new.target_y[n_id[:batch_size]])
                loss.backward()
                
                # Gradient clipping to prevent extreme updates
                if USE_IMPROVED_ATTACK:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()

                total_loss += float(loss)
                total_correct += int(out.argmax(dim=-1).eq(data_new.target_y[n_id[:batch_size]]).sum())

            loss = total_loss / len(target_train_loader)
            approx_acc = total_correct / int(data_new.target_train_mask.sum())

        else:
            # Shadow training
            total_loss = total_correct = 0
            for batch_size, n_id, adjs in shadow_train_loader:
                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.
                adjs = [adj.to(device) for adj in adjs]
                # print("adjs", adjs)

                optimizer.zero_grad()
                out, nodes_and_neighbors = model(data_new.shadow_x[n_id], adjs)
                # print("out traaaaain shadow", out.shape)
                loss = F.nll_loss(out, data_new.shadow_y[n_id[:batch_size]])
                loss.backward()
                
                # Gradient clipping to prevent extreme updates
                if USE_IMPROVED_ATTACK:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()

                total_loss += float(loss)
                total_correct += int(out.argmax(dim=-1).eq(data_new.shadow_y[n_id[:batch_size]]).sum())

            loss = total_loss / len(shadow_train_loader)
            approx_acc = total_correct / int(data_new.shadow_train_mask.sum())

        # print("================ End SAGE Train ==================")
        return approx_acc, loss











    # -----------------------------------------------------------------------------------------------
    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*
    # -------------------------------- TEST FUNCTION -----------------------------------------
    def test(model, isTarget=True):
        model.eval()
        # Also changed this to give true test using full graph. This will give the true train result--No it wont. See comment below

        # This is a better n accurate approach
        if isTarget:

            '''# InTrain for target'''

            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless
            # pred_Intrain = model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask].max(1)[1]
            pred, nodes_and_neighbors = model(data_new.target_x, data_new.target_edge_index)

            pred_Intrain = pred.max(1)[1].to(device)
            # Actual probabilities
            # pred_Intrain_ps = torch.exp(model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask])
            # Apply temperature scaling for better calibration
            pred_scaled = pred / TEMPERATURE if USE_IMPROVED_ATTACK else pred
            pred_Intrain_ps = torch.exp(pred_scaled)
            np.savetxt(save_target_InTrain, pred_Intrain_ps.cpu().detach().numpy())

            # np.save(save_target_InTrain_nodes_neigbors, nodes_and_neighbors)  # Skip for now due to inhomogeneous shape


            # print("End InTrain for target")

            '''# OutTrain for target'''
            # Now using another subgraph for testing rather than full graph
            # pred_out = model(data_new.all_x, data_new.all_edge_index)[data_new.target_test_mask].max(1)[1]


            preds, nodes_and_neighbors = model(data_new.target_test_x, data_new.target_test_edge_index)

            pred_out = preds.max(1)[1].to(device)
            # pred_out_ps = torch.exp(model(data_new.all_x, data_new.all_edge_index)[data_new.target_test_mask])
            # Apply temperature scaling for better calibration
            preds_scaled = preds / TEMPERATURE if USE_IMPROVED_ATTACK else preds
            pred_out_ps = torch.exp(preds_scaled)

            np.savetxt(save_target_OutTrain, pred_out_ps.cpu().detach().numpy())

            # This will allow shuffling of posteriors in the attack model not to loose the node info
            # print("newwwwwwww", nodes_and_neighbors)


            incremented_nodes_and_neighbors = [] #{}
            # for i in range(len(nodes_and_neighbors)):
            #     # print(nodes_and_neighbors[i])  # list
            #     res = [x + num_test_Target for x in nodes_and_neighbors[i]]
            #     incremented_nodes_and_neighbors[i+num_test_Target] = res

            for i in range(len(nodes_and_neighbors)):
                # print(nodes_and_neighbors[i])  # list
                # print(nodes_and_neighbors[i][0])
                # print(nodes_and_neighbors[i][1])

                res = [x + num_test_Target for x in nodes_and_neighbors[i][1]]
                res_0 = nodes_and_neighbors[i][0] + num_test_Target
                incremented_nodes_and_neighbors.append((res_0, res))



            # print("incremented_nodes_and_neighbors", incremented_nodes_and_neighbors)


            # np.save(save_target_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)  # Skip for now due to shape issues

            # print("End OutTrain for target")

            pred_labels = pred_out.tolist()
            # This uses the original target test
            true_labels = data_new.target_test_y  # data_new.all_y[data_new.target_test_mask].tolist() # we can leave this for now or change it ti using the test_target_y

            # The train accuracy is not on the full graph. It's similar to approx_train_acc
            train_acc = get_train_acc(data_new, pred_Intrain)
            # Test n val are on full graph
            test_acc = get_test_acc(data_new, pred_out)

        else:

            '''# InTrain for Shadow'''
            # pred_Intrain = model(data_new.shadow_x, data_new.shadow_edge_index)[data_new.shadow_train_mask].max(1)[1]
            pred, nodes_and_neighbors = model(data_new.shadow_x, data_new.shadow_edge_index)

            pred_Intrain = pred.max(1)[1].to(device)
            # Actual probabilities
            # pred_Intrain_ps = torch.exp(model(data_new.shadow_x, data_new.shadow_edge_index)[data_new.shadow_train_mask])
            # Apply temperature scaling for better calibration
            pred_scaled = pred / TEMPERATURE if USE_IMPROVED_ATTACK else pred
            pred_Intrain_ps = torch.exp(pred_scaled)
            np.savetxt(save_shadow_InTrain, pred_Intrain_ps.cpu().detach().numpy())

            '''# OutTrain for shadow'''
            # Changing to use another subgraph for testing
            # pred_out = model(data_new.all_x, data_new.all_edge_index)[data_new.shadow_test_mask].max(1)[1]
            preds, nodes_and_neighbors = model(data_new.shadow_test_x, data_new.shadow_test_edge_index)

            pred_out = preds.max(1)[1].to(device)
            # pred_out_ps = torch.exp(model(data_new.all_x, data_new.all_edge_index)[data_new.shadow_test_mask])
            # Apply temperature scaling for better calibration
            preds_scaled = preds / TEMPERATURE if USE_IMPROVED_ATTACK else preds
            pred_out_ps = torch.exp(preds_scaled)
            np.savetxt(save_shadow_OutTrain, pred_out_ps.cpu().detach().numpy())

            pred_labels = pred_out.tolist()
            true_labels = data_new.shadow_test_y  # data_new.all_y[data_new.shadow_test_mask].tolist()

            # The train accuracy is not on the full graph. It's similar to approx_train_acc
            train_acc = get_train_acc(data_new, pred_Intrain, False)
            # Test n val are on full graph
            test_acc = get_test_acc(data_new, pred_out, False)

        # pred_Intrain = model(data_new.all_x,data_new.all_edge_index)[data_new.target_train_mask].max(1)[1]
        # # Actual probabilities
        # pred_Intrain_ps = torch.exp(model(data_new.all_x,data_new.all_edge_index)[data_new.target_train_mask])

        # print("posteriors", pred_Intrain)

        # The f1 measures are on test dataset
        f1_marco = get_marco_f1(data_new, pred_labels, true_labels, label_list)
        f1_micro = get_micro_f1(data_new, pred_labels, true_labels, label_list)

        return train_acc, test_acc, f1_marco, f1_micro






    def test_SAGE(model, isTarget=True):
        # print("****************** SAGE test begin *************************")
        model.eval()
        # Also changed this to give true test using full graph. This will give the true train result--No it wont. See comment below

        # This is a better n accurate approach
        if isTarget:

            '''InTrain Target'''
            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless
            # pred_Intrain = model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask].max(1)[1]


            pred = []
            nodes_and_neighbors = []
            total_target_train_correct = 0

            for batch_size, n_id, adjs in target_train_loader:
                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.
                adjs = [adj.to(device) for adj in adjs]
                # print("adjs test", adjs)
                # print("n_id",len(n_id))

                out, node_and_neigh = model(data_new.target_x[n_id], adjs)
                out = torch.exp(out)
                # print("out test", out.shape)
                pred.append(out.cpu())
                nodes_and_neighbors.append(node_and_neigh)

                total_target_train_correct += int(out.argmax(dim=-1).eq(data_new.target_y[n_id[:batch_size]]).sum())

            target_train_acc = total_target_train_correct / int(data_new.target_train_mask.sum())
            # print("target_train_acc", target_train_acc)

            # Need to concat all preds cos it's per batch
            pred_all_inTrain = torch.cat(pred, dim=0)

            # pred, nodes_and_neighbors = model(data_new.target_x,data_new.target_edge_index)
            pred_Intrain = pred_all_inTrain.max(1)[1].to(device) #ensures it is on the same device for working on GPU
            # # Actual probabilities
            # # pred_Intrain_ps = torch.exp(model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask])
            pred_Intrain_ps = pred_all_inTrain  # torch.exp(pred)
            np.savetxt(save_target_InTrain, pred_Intrain_ps.cpu().detach().numpy())
            #
            # np.save(save_target_InTrain_nodes_neigbors, nodes_and_neighbors)  # Skip for now due to inhomogeneous shape
            # print("nodes_and_neighborsnodes_and_neighbors", nodes_and_neighbors) # 630

            # print("End InTrain for target")

            '''OutTrain Target'''

            preds = []
            nodes_and_neighbors = []
            total_target_out_correct = 0
            for batch_size, n_id, adjs in target_test_loader:
                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.
                adjs = [adj.to(device) for adj in adjs]
                # print("adjs test", adjs)
                # print("n_id",len(n_id))

                out, node_and_neigh = model(data_new.target_test_x[n_id], adjs)
                out = torch.exp(out)
                # print("out test", out.shape)
                preds.append(out.cpu())
                nodes_and_neighbors.append(node_and_neigh)

                total_target_out_correct += int(out.argmax(dim=-1).eq(data_new.target_test_y[n_id[:batch_size]]).sum())

            target_out_acc = total_target_out_correct / int(data_new.target_test_mask.sum()) # same as get_train_acc()
            # print("target_out_acc", target_out_acc)

            # Need to concat all preds cos it's per batch
            pred_all_outTrain = torch.cat(preds, dim=0)

            nodes_and_neighbors = list(itertools.chain.from_iterable(nodes_and_neighbors)) #turn list of list into one giant one them
            nodes_and_neighbors = nodes_and_neighbors[
                                  :len_y]

            # print("nodes_and_neighbors", nodes_and_neighbors)

            pred_out = pred_all_outTrain.max(1)[1].to(device)
            pred_out_ps = pred_all_outTrain #torch.exp(preds)
            # print("pred_all_outTrain", pred_all_outTrain)


            np.savetxt(save_target_OutTrain, pred_out_ps.cpu().detach().numpy())
            incremented_nodes_and_neighbors = []  # {}
            # for i in range(len(nodes_and_neighbors)):
            #     # print(nodes_and_neighbors[i])  # list
            #     res = [x + num_test_Target for x in nodes_and_neighbors[i]]
            #     incremented_nodes_and_neighbors[i+num_test_Target] = res

            for i in range(len(nodes_and_neighbors)):
                # print(nodes_and_neighbors[i])  # list
                # print(nodes_and_neighbors[i][0])
                # print(nodes_and_neighbors[i][1])

                res = [x + num_test_Target for x in nodes_and_neighbors[i][1]]
                res_0 = nodes_and_neighbors[i][0] + num_test_Target
                incremented_nodes_and_neighbors.append((res_0, res))

            # np.save(save_target_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)  # Skip for now due to shape issues

            # print("End OutTrain for target")

            pred_labels = pred_out.tolist()
            # This uses the original target test
            true_labels = data_new.target_test_y  # data_new.all_y[data_new.target_test_mask].tolist() # we can leave this for now or change it ti using the test_target_y

            # The train accuracy is not on the full graph. It's similar to approx_train_acc. Changed cos of the train_loader!
            train_acc = get_train_acc(data_new, pred_Intrain)
            # Test n val are on full graph
            test_acc = get_test_acc(data_new, pred_out)

        else:

            '''InTrain Shadow'''
            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless
            # pred_Intrain = model(data_new.shadow_x,data_new.shadow_edge_index)[data_new.shadow_train_mask].max(1)[1]

            pred = []
            nodes_and_neighbors = []
            total_shadow_train_correct = 0
            for batch_size, n_id, adjs in shadow_train_loader:
                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.
                adjs = [adj.to(device) for adj in adjs]
                # print("adjs test", adjs)
                # print("n_id",len(n_id))

                out, node_and_neigh = model(data_new.shadow_x[n_id], adjs)
                out = torch.exp(out)
                # print("out test", out.shape)
                pred.append(out.cpu())
                nodes_and_neighbors.append(node_and_neigh)

                total_shadow_train_correct += int(out.argmax(dim=-1).eq(data_new.shadow_y[n_id[:batch_size]]).sum())

            shadow_train_acc = total_shadow_train_correct / int(data_new.shadow_train_mask.sum())
            # print("shadow_train_acc", shadow_train_acc)

            # Need to concat all preds cos it's per batch
            pred_all_inTrain = torch.cat(pred, dim=0)

            # pred, nodes_and_neighbors = model(data_new.shadow_x,data_new.shadow_edge_index)
            pred_Intrain = pred_all_inTrain.max(1)[1].to(device)
            # # Actual probabilities
            # # pred_Intrain_ps = torch.exp(model(data_new.shadow_x,data_new.shadow_edge_index)[data_new.shadow_train_mask])
            pred_Intrain_ps = pred_all_inTrain  # torch.exp(pred)
            np.savetxt(save_shadow_InTrain, pred_Intrain_ps.cpu().detach().numpy())
            #
            # np.save(save_shadow_InTrain_nodes_neigbors, nodes_and_neighbors)  # Not necsaary
            # print("nodes_and_neighborsnodes_and_neighbors", nodes_and_neighbors) # 630

            # print("End InTrain for shadow")

            '''OutTrain Shadow'''

            preds = []
            nodes_and_neighbors = []

            total_shadow_out_correct = 0
            for batch_size, n_id, adjs in shadow_test_loader:
                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.
                adjs = [adj.to(device) for adj in adjs]
                # print("adjs test", adjs)
                # print("n_id",len(n_id))

                out, node_and_neigh = model(data_new.shadow_test_x[n_id], adjs)
                out = torch.exp(out)
                # print("out test", out.shape)
                preds.append(out.cpu())
                nodes_and_neighbors.append(node_and_neigh)

                total_shadow_out_correct += int(out.argmax(dim=-1).eq(data_new.shadow_test_y[n_id[:batch_size]]).sum())

            shadow_out_acc = total_shadow_out_correct / int(data_new.shadow_test_mask.sum())
            # print("shadow_out_acc", shadow_out_acc)

            # Need to concat all preds cos it's per batch
            pred_all_outTrain = torch.cat(preds, dim=0)

            nodes_and_neighbors = list(itertools.chain.from_iterable(nodes_and_neighbors)) #turn list of list into one giant one them


            nodes_and_neighbors = nodes_and_neighbors[
                                  :len_y]

            # print("nodes_and_neighbors", nodes_and_neighbors)

            pred_out = pred_all_outTrain.max(1)[1].to(device)
            pred_out_ps = pred_all_outTrain #torch.exp(preds)
            # print("pred_all_outTrain", pred_all_outTrain)


            np.savetxt(save_shadow_OutTrain, pred_out_ps.cpu().detach().numpy())

            incremented_nodes_and_neighbors = []  # {}
            # for i in range(len(nodes_and_neighbors)):
            #     # print(nodes_and_neighbors[i])  # list
            #     res = [x + num_test_Shadow for x in nodes_and_neighbors[i]]
            #     incremented_nodes_and_neighbors[i+num_test_Shadow] = res

            for i in range(len(nodes_and_neighbors)):
                # print(nodes_and_neighbors[i])  # list
                # print(nodes_and_neighbors[i][0])
                # print(nodes_and_neighbors[i][1])

                res = [x + num_test_Shadow for x in nodes_and_neighbors[i][1]]
                res_0 = nodes_and_neighbors[i][0] + num_test_Shadow
                incremented_nodes_and_neighbors.append((res_0, res))

            # print("incremented_nodes_and_neighbors", incremented_nodes_and_neighbors)

            # np.save(save_shadow_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)  # Not needed

            # print("End OutTrain for shadow")

            pred_labels = pred_out.tolist()
            # This uses the original shadow test
            true_labels = data_new.shadow_test_y  # data_new.all_y[data_new.shadow_test_mask].tolist() # we can leave this for now or change it ti using the test_shadow_y

            # The train accuracy is not on the full graph. It's similar to approx_train_acc
            train_acc = get_train_acc(data_new, pred_Intrain, False)
            # Test n val are on full graph
            test_acc = get_test_acc(data_new, pred_out, False)

        # print("posteriors", pred_Intrain)

        # The f1 measures are on test dataset
        f1_marco = get_marco_f1(data_new, pred_labels, true_labels, label_list)
        f1_micro = get_micro_f1(data_new, pred_labels, true_labels, label_list)

        # print("****************** SAGE test End *************************")
        return train_acc, test_acc, f1_marco, f1_micro





    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*
    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*
    # --------------------------TRAIN PROCESS-----------------------------------------------
    best_val_acc = best_val_acc = 0

    '''
    Training and testing target and shadow models
    '''

    if data_type.startswith("Custom_"):
        model_training_epoch = DEFAULT_EPOCHS  # Use default epochs for custom datasets
    elif model_type == "SAGE":
        if data_type == "CiteSeer" or data_type == "Cora":
            model_training_epoch = SAGE_EPOCHS_CORA_CITESEER #16 for CiteSeer n Cora, 101 for PubMed, 301 for Flickr n Reddit
        elif data_type == "PubMed":
            model_training_epoch = SAGE_EPOCHS_PUBMED
        else:
            model_training_epoch = SAGE_EPOCHS_FLICKR_REDDIT
    else:
        model_training_epoch = DEFAULT_EPOCHS

    # Target train
    for epoch in range(1, model_training_epoch):
        if model_type == "SAGE":
            approx_train_acc, train_loss = train_SAGE(target_model, target_optimizer)
            train_acc, test_acc, marco, micro = test_SAGE(target_model)
        else:
            approx_train_acc, train_loss = train(target_model, target_optimizer)
            train_acc, test_acc, marco, micro = test(target_model)
        # if val_acc > best_val_acc:
        #     best_val_acc = val_acc
        #     test_acc = tmp_test_acc
        #     marco = tmp_marco
        #     micro = tmp_micro

        # log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'
        # print(log.format(epoch, train_acc, best_val_acc, test_acc,marco,micro))
        log = 'TargetModel Epoch: {:03d}, Approx Train: {:.4f}, Train: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'
        print(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro))
        if epoch == model_training_epoch - 1:
            result_file.write(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro) + "\n")

    print()
    print("=========================================================End Target Train ==============================")

    # shadow train
    for epoch in range(1, model_training_epoch):
        if model_type == "SAGE":
            approx_train_acc, train_loss = train_SAGE(shadow_model, shadow_optimizer, False)
            train_acc, test_acc, marco, micro = test_SAGE(shadow_model, False)
        else:
            approx_train_acc, train_loss = train(shadow_model, shadow_optimizer, False)
            train_acc, test_acc, marco, micro = test(shadow_model, False)
        # if val_acc > best_val_acc:
        #     best_val_acc = val_acc
        #     test_acc = tmp_test_acc
        #     marco = tmp_marco
        #     micro = tmp_micro

        # log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'
        # print(log.format(epoch, train_acc, best_val_acc, test_acc,marco,micro))
        log = 'ShadowModel Epoch: {:03d}, Approx Train: {:.4f}, Train: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'
        print(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro))

        if epoch == model_training_epoch - 1:
            result_file.write(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro) + "\n")

    # pred, nodes_and_neighbors = target_model(data_new.target_x, data_new.target_edge_index)
    # print("single Pred", torch.exp(pred[0])) # this is correct






    ''' =========================================== ATTACK ============================================== '''

    # Use normal python to import posterior files.
    # Split posterior data into 80 / 20 (train, test)

    # add 2 dataset 2 gether pandas

    positive_attack_data = pd.read_csv(save_shadow_InTrain, header=None,
                                       sep=" ")  # dataframe #"posteriorsShadowTrain.txt"

    # target in and out data

    target_data_for_testing_Intrain = pd.read_csv(save_target_InTrain, header=None,
                                                  sep=" ")  # dataframe "posteriorsTargetTrain.txt"
    # Assign 1 to indata
    target_data_for_testing_Intrain["labels"] = 1

    target_data_for_testing_Intrain['nodeID'] = range(0, num_test_Target)

    # print("target_data_for_testing_Intrain.head(10)", target_data_for_testing_Intrain.head(10))
    # print("target_data_for_testing_Intrain.tail(10)", target_data_for_testing_Intrain.tail(10))

    # # randomly select 500
    # chosen_idx = np.random.choice(153431, replace=False, size=50000)
    # print(chosen_idx)
    # positive_attack_data = positive_attack_data.iloc[chosen_idx]

    target_data_for_testing_Outtrain = pd.read_csv(save_target_OutTrain, header=None,
                                                   sep=" ")  # dataframe "posteriorsTargetOut.txt"
    # Assign 0 to outdata
    target_data_for_testing_Outtrain["labels"] = 0

    target_data_for_testing_Outtrain['nodeID'] = range(num_test_Target, num_test_Target+num_test_Target)

    # print(positive_attack_data.head())

    # Assign 1 to training data
    positive_attack_data["labels"] = 1

    print("positive_attack_data.shape", positive_attack_data.shape)

    negative_attack_data = pd.read_csv(save_shadow_OutTrain, header=None, sep=" ")  # "posteriorsShadowOut.txt"

    # # randomly select 140
    # chosen_idx = np.random.choice(55703, replace=False, size=50000)
    # print(chosen_idx)
    # negative_attack_data = negative_attack_data.iloc[chosen_idx]

    # print(negative_attack_data.head())

    # Assign 0 to out data
    negative_attack_data["labels"] = 0
    print("negative_attack_data.shape", negative_attack_data.shape)

    # Combine to single dataframe

    # combine them together
    attack_data_combo = [positive_attack_data, negative_attack_data]
    attack_data = pd.concat(attack_data_combo)

    target_data_for_testing_InAndOutTrain_combo = [target_data_for_testing_Intrain, target_data_for_testing_Outtrain]
    target_data_for_testing_InAndOutTrain = pd.concat(target_data_for_testing_InAndOutTrain_combo)

    print("attack_data.shape", attack_data.shape)
    # print(attack_data.head())

    # # sample randomly
    # # returns all but in a random fashion
    # attack_data = attack_data.sample(frac=1)
    # print(attack_data.head())

    X_attack = attack_data.drop("labels", axis=1)
    print("X_attack.shape", X_attack.shape)

    y_attack = attack_data["labels"]

    # let's do in and out for attack data (shadow)
    X_attack_InTrain = positive_attack_data.drop("labels", axis=1)
    y_attack_InTrain = positive_attack_data["labels"]

    X_attack_OutTrain = negative_attack_data.drop("labels", axis=1)
    y_attack_OutTrain = negative_attack_data["labels"]

    print("X_attack_InTrain", X_attack_InTrain.shape)
    print("X_attack_OutTrain", X_attack_OutTrain.shape)
    
    # Check class distribution in attack training data
    print(f"Attack training data - Class 1 (members): {len(y_attack_InTrain)}, Class 0 (non-members): {len(y_attack_OutTrain)}")
    print(f"Class ratio: {len(y_attack_InTrain) / (len(y_attack_InTrain) + len(y_attack_OutTrain)):.2f} members")

    # For in train data (target)
    X_InTrain = target_data_for_testing_Intrain.drop(["labels", "nodeID"], axis=1)
    y_InTrain = target_data_for_testing_Intrain["labels"]
    nodeID_InTrain = target_data_for_testing_Intrain["nodeID"]


    # For Out train data
    X_OutTrain = target_data_for_testing_Outtrain.drop(["labels", "nodeID"], axis=1)
    y_OutTrain = target_data_for_testing_Outtrain["labels"]
    nodeID_OutTrain = target_data_for_testing_Outtrain["nodeID"]

    # For in out data
    X_InOutTrain = target_data_for_testing_InAndOutTrain.drop(["labels", "nodeID"], axis=1)
    print("X_InTrain.shape", X_InOutTrain.shape)

    y_InOutTrain = target_data_for_testing_InAndOutTrain["labels"]
    nodeID_InOutTrain = target_data_for_testing_InAndOutTrain["nodeID"]

    # convert to numpy
    # for shadow
    X_attack_InOut, y_attack_InOut = X_attack.to_numpy(), y_attack.to_numpy()

    X_attack_InTrain, X_attack_OutTrain = X_attack_InTrain.to_numpy(), X_attack_OutTrain.to_numpy()
    y_attack_InTrain, y_attack_OutTrain = y_attack_InTrain.to_numpy(), y_attack_OutTrain.to_numpy()

    # for target
    X_InTrain, y_InTrain, nodeID_InTrain = X_InTrain.to_numpy(), y_InTrain.to_numpy(), nodeID_InTrain.to_numpy()
    X_OutTrain, y_OutTrain, nodeID_OutTrain  = X_OutTrain.to_numpy(), y_OutTrain.to_numpy(), nodeID_OutTrain.to_numpy()

    # for target
    X_InOutTrain, y_InOutTrain, nodeID_InOutTrain = X_InOutTrain.to_numpy(), y_InOutTrain.to_numpy(), nodeID_InOutTrain.to_numpy()

    # # Plot graphs
    #
    # plt.imshow(X_attack_InTrain, interpolation='nearest', aspect='auto')
    # plt.colorbar()
    # plt.tight_layout()
    # plt.title('Positive: In Train Posteriors')
    # plt.show()
    #
    # plt.imshow(X_attack_OutTrain, interpolation='nearest', aspect='auto')
    # plt.colorbar()
    # plt.tight_layout()
    # plt.title('Negative: Out Train Posteriors')
    # plt.show()
    #
    #
    # plt.imshow(X_InTrain, interpolation='nearest', aspect='auto')
    # plt.colorbar()
    # plt.tight_layout()
    # plt.title('Positive: Target Posteriors')
    # plt.show()
    #
    # plt.imshow(X_OutTrain, interpolation='nearest', aspect='auto')
    # plt.colorbar()
    # plt.tight_layout()
    # plt.title('Negative: Target Posteriors')
    # plt.show()

    attack_train_data_X, attack_test_data_X, attack_train_data_y, attack_test_data_y = train_test_split(X_attack,
                                                                                                        y_attack,
                                                                                                        test_size=50,
                                                                                                        stratify=y_attack,
                                                                                                        random_state=rand_state)
    # print("baba")

    # convert series data to numpy array
    attack_train_data_X, attack_test_data_X, attack_train_data_y, attack_test_data_y = attack_train_data_X.to_numpy(), attack_test_data_X.to_numpy(), attack_train_data_y.to_numpy(), attack_test_data_y.to_numpy()

    # print("Attack data printing...")
    # print(attack_test_data_X, attack_test_data_y)

    # Attack_train
    attack_train_data = torch.utils.data.TensorDataset(torch.from_numpy(attack_train_data_X).float(), torch.from_numpy(
        attack_train_data_y))  # convert to float to fix  uint8_t overflow error
    attack_train_data_loader = torch.utils.data.DataLoader(attack_train_data, batch_size=ATTACK_BATCH_SIZE, shuffle=True)

    # Attack_test = combo of targettrain and targetOut
    attack_test_data = torch.utils.data.TensorDataset(torch.from_numpy(attack_test_data_X).float(), torch.from_numpy(
        attack_test_data_y))  # convert to float to fix  uint8_t overflow error
    attack_test_data_loader = torch.utils.data.DataLoader(attack_test_data, batch_size=ATTACK_BATCH_SIZE, shuffle=True)


    # Training InData
    target_data_for_testing_InTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_InTrain).float(),
                                                                          torch.from_numpy(y_InTrain), torch.from_numpy(nodeID_InTrain))
    target_data_for_testing_InTrain_data_loader = torch.utils.data.DataLoader(target_data_for_testing_InTrain_data,
                                                                              batch_size=ATTACK_TEST_BATCH_SIZE, shuffle=False)

    # Training OutData
    target_data_for_testing_OutTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_OutTrain).float(),
                                                                           torch.from_numpy(y_OutTrain), torch.from_numpy(nodeID_OutTrain))
    target_data_for_testing_OutTrain_data_loader = torch.utils.data.DataLoader(target_data_for_testing_OutTrain_data,
                                                                               batch_size=ATTACK_TEST_BATCH_SIZE, shuffle=False)

    # Training InOut Data
    target_data_for_testing_InOutTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_InOutTrain).float(),
                                                                             torch.from_numpy(y_InOutTrain), torch.from_numpy(nodeID_InOutTrain))
    target_data_for_testing_InOutTrain_data_loader = torch.utils.data.DataLoader(
        target_data_for_testing_InOutTrain_data,
        batch_size=ATTACK_TEST_BATCH_SIZE, shuffle=True)

    # features, labels = next(iter(attack_test_data_loader))
    # print(features, labels)

    class AttackModel(nn.Module):
        """Old attack model (kept for reference)"""
        def __init__(self):
            super().__init__()

            # inputs to hidden layer linear transformation
            # Note, when using Linear, weight and biases are randomly initialized for you
            self.hidden = nn.Linear(dataset.num_classes, 100)
            self.hidden2 = nn.Linear(100, 50)
            # output layer, 10 units - one for each digits
            self.output = nn.Linear(50, 2)

            # # Define sigmoid activation and softmax output
            # # comment this cos u can just define directly if u are using functional
            # self.sigmoid = nn.Sigmoid()
            # self.softmax = nn.Softmax(dim=1)

        def forward(self, x):
            # # pass the input tensor through each of our operations
            # # comment to use functional
            # x = self.hidden(x)
            # x = self.sigmoid(x)
            # x = self.output(x)
            # x = self.softmax(x)

            # Hidden layer with sigmoid activation
            x = F.sigmoid(self.hidden(x))
            x = F.sigmoid(self.hidden2(x))
            # output layer with softmax activation
            x = F.softmax(self.output(x), dim=1)
            # print("xxxxxxxx", x)

            return x


    class Net(nn.Module):
        """Original simple attack model"""
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = nn.Linear(dataset.num_classes, 100)
            self.fc2 = nn.Linear(100, 50)
            self.fc3 = nn.Linear(50, 2)
            self.softmax = nn.Softmax(dim=1)

        def forward(self, X):
            # print("attack X",X)
            X = F.relu(self.fc1(X))
            X = F.relu(self.fc2(X))
            X = self.fc3(X)
            X = self.softmax(X)

            return X
    
    class ImprovedAttackModel(nn.Module):
        """Enhanced attack model with better architecture for membership inference"""
        def __init__(self, input_dim=None):
            super(ImprovedAttackModel, self).__init__()
            input_size = input_dim if input_dim else dataset.num_classes
            
            # Deeper network with batch normalization and dropout
            self.fc1 = nn.Linear(input_size, 256)
            self.bn1 = nn.BatchNorm1d(256)
            self.dropout1 = nn.Dropout(ATTACK_DROPOUT)
            
            self.fc2 = nn.Linear(256, 128)
            self.bn2 = nn.BatchNorm1d(128)
            self.dropout2 = nn.Dropout(ATTACK_DROPOUT)
            
            self.fc3 = nn.Linear(128, 64)
            self.bn3 = nn.BatchNorm1d(64)
            self.dropout3 = nn.Dropout(ATTACK_DROPOUT * 0.7)  # Less dropout in later layers
            
            self.fc4 = nn.Linear(64, 32)
            self.bn4 = nn.BatchNorm1d(32)
            
            self.fc5 = nn.Linear(32, 2)
            
            # Initialize weights with better strategy
            self._initialize_weights()
            
        def _initialize_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm1d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
        
        def forward(self, x):
            # Layer 1
            x = self.fc1(x)
            x = self.bn1(x)
            x = F.relu(x)
            x = self.dropout1(x)
            
            # Layer 2
            x = self.fc2(x)
            x = self.bn2(x)
            x = F.relu(x)
            x = self.dropout2(x)
            
            # Layer 3
            x = self.fc3(x)
            x = self.bn3(x)
            x = F.relu(x)
            x = self.dropout3(x)
            
            # Layer 4
            x = self.fc4(x)
            x = self.bn4(x)
            x = F.relu(x)
            
            # Output layer
            x = self.fc5(x)
            
            # No softmax here - use with CrossEntropyLoss
            return x


    def init_weights(m):
        if type(m) == nn.Linear:
            torch.nn.init.xavier_uniform(m.weight)
            m.bias.data.fill_(0.01)


    # create the ntwk - use improved model for better performance
    USE_IMPROVED_MODEL = USE_IMPROVED_ATTACK  # Use improved model if attack improvements are enabled
    
    if USE_IMPROVED_MODEL:
        attack_model = ImprovedAttackModel()
        print("Using ImprovedAttackModel with BatchNorm and Dropout")
    else:
        attack_model = Net()  # Original model
        attack_model.apply(init_weights)  # initialize weight rather than randomly
        print("Using original Net model")
    
    attack_model = attack_model.to(device)
    print(attack_model)


    def attack_train(model, trainloader, testloader, criterion, optimizer, epochs, steps=0, scheduler=None):
        # train ntwk

        # Decay LR by a factor of 0.1 every 7 epochs
        # scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

        final_train_loss = 0
        train_losses, test_losses = [], []
        posteriors = []
        
        # Early stopping parameters
        best_test_loss = float('inf')
        patience = 15 if USE_IMPROVED_ATTACK else 10
        patience_counter = 0
        best_model_state = None
        
        for e in range(epochs):
            running_loss = 0
            train_accuracy = 0

            # This is features, labels cos we dont care about nodeID during training! only during test
            for features, labels in trainloader:
                model.train()
                features, labels = features.to(device), labels.to(device)

                # print("post shape", features.shape)
                # print("labels",labels)
                optimizer.zero_grad()
                # print("features", features.shape)

                # features = features.unsqueeze(1) #unsqueeze
                # flatten features
                features = features.view(features.shape[0], -1)

                logps = model(features)  # log probabilities
                # print("labelsssss", labels.shape)
                loss = criterion(logps, labels)

                # Actual probabilities
                # For ImprovedAttackModel, apply softmax to get probabilities
                if USE_IMPROVED_MODEL:
                    ps = F.softmax(logps, dim=1)
                else:
                    ps = logps  # Original model already has softmax

                top_p, top_class = ps.topk(1,
                                           dim=1)  # top_p gives the probabilities while top_class gives the predicted classes
                # print(top_p)
                equals = top_class == labels.view(
                    *top_class.shape)  # making the shape of the label and top class the same
                train_accuracy += torch.mean(equals.type(torch.FloatTensor))

                loss.backward()
                
                # Gradient clipping to prevent extreme updates
                if USE_IMPROVED_ATTACK:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()

                running_loss += loss.item()
            else:
                # Everything in this else block executes after every epock
                # print(f"training loss: {running_loss}")

                # test_loss = 0
                # test_accuracy = 0
                #
                # # Turn off gradients for validation, saves memory and computations
                # with torch.no_grad():
                #     # Doing validation
                #
                #     # set model to evaluation mode
                #     model.eval()
                #
                #     if e == epochs - 1:
                #         print("Doing attack validation===========")
                #     # validation pass
                #
                #     for features, labels in testloader:
                #         # features = features.unsqueeze(1)  # unsqueeze
                #         features = features.view(features.shape[0], -1)
                #         logps = model(features)
                #         test_loss += criterion(logps, labels)
                #
                #         # Actual probabilities
                #         ps = torch.exp(logps)
                #
                #         top_p, top_class = ps.topk(1,
                #                                    dim=1)  # top_p gives the probabilities while top_class gives the predicted classes
                #         # print(top_p)
                #         equals = top_class == labels.view(
                #             *top_class.shape)  # making the shape of the label and top class the same
                #         test_accuracy += torch.mean(equals.type(torch.FloatTensor))
                test_loss, test_accuracy, _, _, _, _, _, _, _ = attack_test(model, testloader, trainTest=True)

                # set model back yo train model
                model.train()
                # scheduler.step()

                train_losses.append(running_loss / len(trainloader))
                test_losses.append(test_loss)

                # get final train loss. To be returned at the end of the training loop
                final_train_loss = running_loss / len(trainloader)

                print("Epoch: {}/{}..".format(e + 1, epochs),
                      "Training loss: {:.5f}..".format(running_loss / len(trainloader)),
                      "Test Loss: {:.5f}..".format(test_loss),
                      "Train Accuracy: {:.3f}".format(train_accuracy / len(trainloader)),
                      "Test Accuracy: {:.3f}".format(test_accuracy)
                      )
                
                # Step the scheduler if provided
                if scheduler is not None:
                    scheduler.step(test_loss)
                
                # Early stopping logic
                if USE_IMPROVED_ATTACK:
                    if test_loss < best_test_loss:
                        best_test_loss = test_loss
                        best_model_state = model.state_dict().copy()
                        patience_counter = 0
                    else:
                        patience_counter += 1
                        if patience_counter >= patience:
                            print(f"Early stopping at epoch {e+1} with best test loss: {best_test_loss:.5f}")
                            model.load_state_dict(best_model_state)
                            break

        # # plot train and test loss
        # plt.show()
        # plt.plot(train_losses)
        # plt.plot(test_losses)
        # plt.title('Model Losses')
        # plt.ylabel('loss')
        # plt.xlabel('epoch')
        # plt.legend(['train', 'val'], loc='upper left')
        # plt.show()

        return final_train_loss


    def attack_test(model, testloader, singleClass=False, trainTest=False):
        test_loss = 0
        test_accuracy = 0
        auroc = 0
        precision = 0
        recall = 0
        f_score = 0

        posteriors = []
        all_nodeIDs = []
        true_predicted_nodeIDs_and_class = {}
        false_predicted_nodeIDs_and_class = {}

        # Turn off gradients for validation, saves memory and computations
        with torch.no_grad():
            # Doing validation

            # set model to evaluation mode
            model.eval()

            if trainTest:
                for features, labels in testloader:
                    features, labels = features.to(device), labels.to(device)
                    # features = features.unsqueeze(1)  # unsqueeze
                    features = features.view(features.shape[0], -1)
                    logps = model(features)
                    test_loss += criterion(logps, labels)

                    # Actual probabilities
                    if USE_IMPROVED_MODEL:
                        ps = F.softmax(logps, dim=1)
                    else:
                        ps = logps  # Original model already has softmax
                    posteriors.append(ps)

                    # if singleclass=false
                    if not singleClass:
                        y_true = labels.cpu().unsqueeze(-1)
                        # print("y_true", y_true)
                        y_pred = ps.argmax(dim=-1, keepdim=True)
                        # print("y_pred", y_pred)

                        # uncomment this to show AUROC
                        auroc += roc_auc_score(y_true.cpu().numpy(), y_pred.cpu().numpy())
                        # print("auroc", auroc)

                        precision += precision_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted', zero_division=0)

                        recall += recall_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')

                        f_score += f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')

                    top_p, top_class = ps.topk(1,
                                               dim=1)  # top_p gives the probabilities while top_class gives the predicted classes
                    # print(top_p)
                    equals = top_class == labels.view(
                        *top_class.shape)  # making the shape of the label and top class the same
                    test_accuracy += torch.mean(equals.type(torch.FloatTensor))

            else:
                for features, labels, nodeIDs in testloader:
                    features, labels = features.to(device), labels.to(device)
                    # features = features.unsqueeze(1)  # unsqueeze
                    features = features.view(features.shape[0], -1)
                    logps = model(features)
                    test_loss += criterion(logps, labels)

                    # Actual probabilities
                    if USE_IMPROVED_MODEL:
                        ps = F.softmax(logps, dim=1)
                    else:
                        ps = logps  # Original model already has softmax
                    posteriors.append(ps)

                    # print("ps", ps)
                    # print("nodeIDs", nodeIDs)

                    all_nodeIDs.append(nodeIDs)

                    # if singleclass=false
                    if not singleClass:
                        y_true = labels.cpu().unsqueeze(-1)
                        # print("y_true", y_true)
                        y_pred = ps.argmax(dim=-1, keepdim=True)
                        # print("y_pred", y_pred)

                        # uncomment this to show AUROC
                        auroc += roc_auc_score(y_true.cpu().numpy(), y_pred.cpu().numpy())
                        # print("auroc", auroc)

                        precision += precision_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted', zero_division=0)

                        recall += recall_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')

                        f_score += f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')

                    top_p, top_class = ps.topk(1, dim=1)  # top_p gives the probabilities while top_class gives the predicted classes
                    # print("top_p", top_p)
                    # print("top_class", top_class)

                    equals = top_class == labels.view(*top_class.shape)  # making the shape of the label and top class the same
                    # print("equals", equals)
                    for i in range(len(equals)):
                        if equals[i]:
                            # print("baba")
                            # print("true pred nodeIDs", nodeIDs[i])

                            true_predicted_nodeIDs_and_class[nodeIDs[i].item()] = top_class[i].item()
                        else:
                            false_predicted_nodeIDs_and_class[nodeIDs[i].item()] = top_class[i].item()



                    test_accuracy += torch.mean(equals.type(torch.FloatTensor))



        test_accuracy = test_accuracy / len(testloader)
        test_loss = test_loss / len(testloader)
        final_auroc = auroc / len(testloader)
        final_precision = precision / len(testloader)
        final_recall = recall / len(testloader)
        final_f_score = f_score / len(testloader)

        # print('final precision', final_precision)
        # print('final micro precision', final_recall)

        # print("final auroc", final_auroc)
        # if all_nodeIDs:
        #     print("all_nodeIDs", torch.cat(all_nodeIDs, 0), "shape Cat", torch.cat(all_nodeIDs, 0).shape)
        #     # true_predicted_nodeIDs_and_class = torch.cat(true_predicted_nodeIDs_and_class)

        return test_loss, test_accuracy, posteriors, final_auroc, final_precision, final_recall, final_f_score, true_predicted_nodeIDs_and_class, false_predicted_nodeIDs_and_class


    '''Initialization / params for attack model'''

    # Use label smoothing instead of class weighting for better generalization
    class LabelSmoothingCrossEntropy(nn.Module):
        def __init__(self, smoothing=0.1):
            super().__init__()
            self.smoothing = smoothing
            
        def forward(self, pred, target):
            n_classes = pred.size(-1)
            log_preds = F.log_softmax(pred, dim=-1)
            loss = -log_preds.gather(dim=-1, index=target.unsqueeze(1))
            loss = loss.squeeze(1)
            smooth_loss = -log_preds.mean(dim=-1)
            loss = (1.0 - self.smoothing) * loss + self.smoothing * smooth_loss
            return loss.mean()
    
    # Use label smoothing to prevent overconfident predictions
    if USE_IMPROVED_ATTACK:
        criterion = LabelSmoothingCrossEntropy(smoothing=0.1)
        print("Using label smoothing with smoothing=0.1")
    else:
        criterion = nn.CrossEntropyLoss()  # Standard loss for original model

    # Enhanced optimizer with weight decay for regularization
    optimizer = torch.optim.Adam(attack_model.parameters(), lr=ATTACK_LR, weight_decay=ATTACK_WEIGHT_DECAY)
    
    # Learning rate scheduler for better convergence
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    
    epochs = ATTACK_EPOCHS

    '''==============Train and test Attack model ========== '''

    attack_train(attack_model, attack_train_data_loader, attack_test_data_loader, criterion, optimizer, epochs, scheduler=scheduler if USE_IMPROVED_MODEL else None)

    # test to confirm using attack_test_data_loader
    _, test_accuracyConfirmTest, posteriors, auroc, precision, recall, f_score, _, _ = attack_test(attack_model,
                                                                                             attack_test_data_loader, trainTest=True)
    # print(posteriors)

    # This is d result on the test set we used i.e split the attack data into train and test.
    # Size of the test = 50
    print("To confirm using attack_test_data_loader (50 test samples): {:.3f}".format(test_accuracyConfirmTest),
          "AUROC: {:.3f}".format(auroc), "precision: {:.3f}".format(precision), "recall {:.3f}".format(recall))

    # test for InOut train target data
    # This is the one we are interested in
    _, test_accuracyInOut, posteriors, auroc, precision, recall, f_score, true_predicted_nodeIDs_and_class, false_predicted_nodeIDs_and_class = attack_test(attack_model,
                                                                                       target_data_for_testing_InOutTrain_data_loader)
    # # print(posteriors)
    # print("true_predicted_nodeIDs_and_class", true_predicted_nodeIDs_and_class)
    # print("false_predicted_nodeIDs_and_class", false_predicted_nodeIDs_and_class)

    print("Test accuracy with Target Train InOut: {:.3f}".format(test_accuracyInOut), "AUROC: {:.3f}".format(auroc),
          "precision: {:.3f}".format(precision), "recall {:.3f}".format(recall), "F1 score {:.3f}".format(f_score),
          "===> Attack Performance!")

    result_file.write(
        "Test accuracy with Target Train InOut: {:.3f} ".format(test_accuracyInOut) + " AUROC: {:.3f}".format(auroc) +
        " precision: {:.3f}".format(precision) + " recall {:.3f}".format(recall) + " F1 score {:.3f}".format(f_score) +
        "===> Attack Performance! \n")

    # test for Only In train target data
    _, test_accuracyIn, posteriors, _, precision, recall, f_score, _, _ = attack_test(attack_model,
                                                                                target_data_for_testing_InTrain_data_loader,
                                                                                True)
    # print("Test accuracy with Target Train In: {:.3f}".format(test_accuracyIn), "precision: {:.3f}".format(precision),
    #       "recall {:.3f}".format(recall), "F1 score {:.3f}".format(f_score))

    # test for Only Out train target data
    _, test_accuracyOut, posteriors, _, precision, recall, f_score, _, _ = attack_test(attack_model,
                                                                                 target_data_for_testing_OutTrain_data_loader,
                                                                                 True)
    # print("Test accuracy with Target Train Out: {:.3f}".format(test_accuracyOut), "precision: {:.3f}".format(precision),
    #       "recall {:.3f}".format(recall), "F1 score {:.3f}".format(f_score))

    result_file.write(
        "Test accuracy with Target Train In: {:.3f} ".format(
            test_accuracyIn) + " |=====| Test accuracy with Target Train Out: {:.3f}".format(test_accuracyOut) + "\n")

    print("data_type", data_type)
    print("model_type", model_type)
    end_time = time.time()

    total_time = round(end_time - start_time, 3)
    print("WhichRun", which_run, " Total time", total_time)

    # result_file.write("Data:"+data_type +" Model:"+ model_type+"\n\n\n")
    result_file.write(" ================ WhichRun: " + str(
        which_run) + " || Data: " + data_type + " || Model: " + model_type + " || Time: " + str(
        total_time) + " || rand_state: " + str(rand_state) + " ================== \n\n\n")

    result_file.close()
