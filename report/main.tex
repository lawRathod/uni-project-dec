\documentclass{article}

% Recommended packages
\usepackage[utf8]{inputenc} % Allows UTF-8 input
\usepackage[T1]{fontenc}    % Use modern font encodings
\usepackage{amsmath}    % For mathematical formulas
\usepackage{amssymb}    % For additional mathematical symbols
\usepackage{amsthm}     % For theorem-like environments
\usepackage{graphicx}   % For including images
\usepackage{hyperref}   % For clickable links in the PDF
\hypersetup{
    colorlinks=true,  % Colors the links instead of boxing them
    linkcolor=blue,   % Color for internal links (e.g., cross-references)
    urlcolor=cyan,    % Color for external URLs
    filecolor=magenta % Color for links to local files
}
\usepackage{cite}       % For managing citations
\usepackage{geometry}   % For setting page margins
\geometry{a4paper, margin=0.8in} % Example margin settings

% Define title and author
\title{Membership Inference Attacks on Graph Neural Networks Using Synthetic Data: A Privacy Vulnerability Analysis\footnote{Code repository: \url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec}}}

\author{Prateek Rathod\\
Department of Computer Science\\
Master's Program in Computer Science}

\date{\today}
\begin{document}

\maketitle

\begin{abstract}
Membership Inference Attacks (MIAs) create privacy risks for machine learning models by finding out if specific data points were used during training. This project studies an attack against Graph Neural Networks (GNNs) that uses synthetic graph data. This removes the attacker's need to access real training data. This work adapts existing MIA methods to work with synthetic graphs generated through the DLGrapher project\cite{dlgrapher2022}. The study evaluates attack success on four GNN types: Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), GraphSAGE, and Simple Graph Convolution (SGC). 

The experiments use real-world data from Twitch and Event platforms. The results show that synthetic data can work for membership inference attacks but performs worse than using real data for shadow model training. This work introduces the TSTS (Train on Subgraph, Test on Subgraph) method to handle computational limits while keeping attacks working. Results show big differences in how vulnerable each model is. SGC is most vulnerable, GraphSAGE and GAT are moderately vulnerable, while GCN is most resistant with nearly random results. 

These findings reveal important privacy problems in GNN use and emphasize the need for privacy protection beyond just limiting data access. This work helps understand the trade-off between privacy and usefulness in graph learning systems and gives ideas for building strong defenses against membership inference attacks.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
Membership Inference Attacks (MIAs) are a serious privacy problem in machine learning systems. They let attackers find out if specific data points were used during model training \cite{shokri2017membership}. This creates big risks in areas like healthcare, finance, and social networks, where training data often has personal information. Traditional MIA methods need access to real training data. This project looks at a new type of attack that uses synthetic graph data instead.

Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data. They are used in social network analysis, recommendation systems, and biological networks \cite{kipf2017semi, velickovic2018graph, hamilton2017inductive}. However, limited research exists on their vulnerability to membership inference attacks \cite{he2021membership}, especially when attackers can't access the original data. This project fills this gap by developing and evaluating MIA methods that use synthetic graph data. The work utilizes synthetic graph structures provided by the DLGrapher project \cite{dlgrapher2022} that resemble real-world data.

The main contributions of this work are: (1) adapting existing MIA methods to work with synthetic graph data, (2) evaluating multiple GNN types including Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), GraphSAGE, and Simple Graph Convolution (SGC), (3) developing enhanced feature engineering methods for improved attack performance, and (4) conducting experiments using real-world data from Twitch and Event platforms. The results demonstrate that synthetic data can replace real training samples in shadow model building, achieving attack success rates similar to traditional methods while requiring substantially less attacker knowledge.

\section{Background}
\subsection{Membership Inference Attacks}
Membership Inference Attacks use the fact that machine learning models often overfit their training data. This creates different patterns between members (training samples) and non-members (unseen samples) \cite{salem2019ml, nasr2019comprehensive}. The attack has three main parts:

\textbf{Target Model:} The victim GNN model trained on private graph data that the attacker wants to attack. This model creates probability outputs that accidentally leak membership information through small statistical patterns.

\textbf{Shadow Models:} Models that are similar to the target model, trained on data that should follow similar patterns. These models copy the target's behavior. This lets the attacker see membership patterns without direct access to the target's training process.

\textbf{Attack Model:} A binary classifier trained to tell the difference between members and non-members based on model outputs. It learns decision rules from shadow model behaviors and uses them on target model outputs.

\subsection{Graph Neural Networks}
GNNs extend deep learning to graph data by combining information from node neighborhoods through message-passing mechanisms. Unlike traditional neural networks that work with grid-structured data, GNNs handle irregular graph topologies where each node can have different numbers of connections. This flexibility makes them useful for analyzing social networks, biological systems, and knowledge graphs where relationships are important.

The core principle behind GNNs is iterative message passing, where nodes aggregate information from their local neighborhoods to update their representations. Each layer refines node embeddings by incorporating structural and feature information from connected nodes. This process creates node representations that capture both local neighborhood patterns and global graph structure through multiple aggregation layers.

The models evaluated in this work represent different approaches to this message-passing framework:

\textbf{GCN:} Uses spectral graph convolutions with localized filters that operate in the spatial domain. The mathematical formulation $H^{(l+1)} = \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})$ applies symmetric normalization to the adjacency matrix, where $\tilde{A} = A + I$ includes self-connections and $\tilde{D}$ is the degree matrix \cite{kipf2017semi}. This normalization prevents the vanishing gradient problem while ensuring stable training dynamics.

\textbf{GAT:} Uses attention mechanisms to learn adaptive importance weights for each neighbor connection. The attention coefficients $\alpha_{ij} = \text{softmax}_j(\text{LeakyReLU}(a^T[Wh_i||Wh_j]))$ allow the model to focus on relevant neighbors while ignoring less important connections \cite{velickovic2018graph}. This selective attention enables better handling of noisy or irrelevant edges in real-world graphs.

\textbf{GraphSAGE:} Addresses scalability challenges by sampling fixed-size neighborhoods rather than using all neighbors. It combines features through learnable aggregation functions including mean pooling, LSTM-based sequential processing, or max pooling operations \cite{hamilton2017inductive}. This sampling approach enables training on large graphs that would otherwise exceed memory limits.

\textbf{SGC:} Simplifies the GCN architecture by removing nonlinear activation functions between layers, reducing computational complexity to $Y = \text{softmax}(\tilde{A}^KXW)$ where $K$ consecutive graph convolutions are collapsed into a single operation \cite{wu2019simplifying}. This simplification maintains performance while reducing training time and memory requirements.

Each architecture\footnote{GNN implementations in TSTS.py: \url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/blob/main/rebMIGraph/TSTS.py\#L317-361}} represents different trade-offs between expressiveness, computational efficiency, and scalability. These design choices directly impact both model performance and privacy vulnerability, as shown through our experimental analysis of membership inference susceptibility.

\section{Methodology}
This project introduces a new approach to membership inference attacks by using pre-generated synthetic graph data from the DLGrapher project. This removes the attacker's need for real training data access. The proposed method differs from traditional MIA approaches through its unique data handling and attack process.

\subsection{Attack Architecture}
The attack framework maintains the three-model structure while incorporating pre-existing synthetic data as a key component. The target model works on real graph data, while shadow models only use synthetic graphs provided by DLGrapher that mimic the original data patterns. This difference tests whether similar data patterns are enough for successful membership inference.

\subsection{Data Pipeline}
The computational limits of synthetic graph generation are addressed through a subgraph-based approach:

\textbf{Data Partitioning:} Each dataset has three parts: (1) training subgraphs for target model training, (2) non-training subgraphs from the same data pattern for testing, and (3) synthetic subgraphs provided by the DLGrapher project. Each part has 256 subgraphs with 120 nodes, ensuring minimal overlap between subgraphs.

\textbf{Bridge Module:} A custom data loader\footnote{\url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/blob/main/rebMIGraph/bridge.py}} handles binary pickle files containing (DataFrame, NetworkX Graph) pairs, converting them to PyTorch Geometric Data objects that work with GNN implementations.

\textbf{Adapter Module:} Implements the TSTS (Train on Subgraph, Test on Subgraph) method\footnote{\url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/blob/main/rebMIGraph/rebmi_adapter.py}}, managing data splits and making sure target and shadow model datasets are properly separated.

\subsection{Attack Execution Pipeline}
The attack implementation\footnote{Main attack code: \url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/blob/main/rebMIGraph/TSTS.py}} works through seven steps:
\begin{enumerate}
\item \textbf{Data Preparation:} Load and prepare graph data from pickle files
\item \textbf{Target Model Training:} Train GNN on real training subgraphs
\item \textbf{Shadow Model Training:} Train same architecture on synthetic subgraphs provided by DLGrapher
\item \textbf{Feature Engineering:} Extract and improve graph features including degree centrality, clustering coefficients, and spectral properties
\item \textbf{Attack Model Training:} Train binary classifier on shadow model outputs with known membership labels
\item \textbf{Attack Execution:} Apply trained attack model to target model outputs
\item \textbf{Performance Evaluation:} Calculate accuracy, AUROC, precision, recall, and F1 scores
\end{enumerate}

\section{Implementation Details}
\subsection{Datasets}
This study employs two real-world social network datasets, each with unique features and challenges for membership inference:

\textbf{Twitch Dataset:} Comes from the Twitch streaming platform, containing user interaction graphs with features including \texttt{views}, \texttt{mature}, \texttt{life\_time}, \texttt{created\_at}, \texttt{updated\_at}, \texttt{dead\_account}, \texttt{language}, and \texttt{affiliate} (target variable). The dataset captures social dynamics and content consumption patterns.

\textbf{Event Dataset:} Comes from event-based social platforms, featuring user attributes such as \texttt{locale}, \texttt{birthyear}, \texttt{gender} (target variable), \texttt{joinedAt}, and \texttt{timezone}. This dataset represents demographic-based social connections.

Each dataset is prepared into 256 subgraphs containing 120 nodes with minimal overlap between subgraphs, stored as pickle files containing (pandas.DataFrame, networkx.Graph) pairs. The binary classification task matches the original MIA framework requirements.

\subsection{Data Integration Pipeline}
The \texttt{bridge.py} module\footnote{\url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/blob/main/rebMIGraph/bridge.py\#L28-50}} handles data format conversion, changing pickle-stored subgraphs into PyTorch Geometric Data objects. Key implementation:
\begin{verbatim}
def convert_to_pyg(self, df, graph):
"""Convert (DataFrame, Graph) tuple to torch_geometric.Data"""
# Convert networkx to torch_geometric
data = from_networkx(graph)

# Add node features from DataFrame
if 'x' not in data:
    # Use all numeric columns except target as features
    feature_cols = [col for col in df.columns if col not in ['affiliate', 'gender'] and df[col].dtype in ['int64', 'float64']]
    data.x = torch.tensor(df[feature_cols].values, dtype=torch.float)

# Add target labels
if 'affiliate' in df.columns:  # Twitch dataset
    data.y = torch.tensor(df['affiliate'].values, dtype=torch.long)
elif 'gender' in df.columns:  # Event dataset
    # Convert string gender to numeric
    if df['gender'].dtype.name == 'category' or df['gender'].dtype == object:
        gender_map = {'male': 0, 'female': 1}
        data.y = torch.tensor(df['gender'].map(gender_map).values, dtype=torch.long)
    else:
        data.y = torch.tensor(df['gender'].values, dtype=torch.long)
return data
\end{verbatim}

\subsection{Feature Engineering}
The \texttt{rebmi\_adapter.py} module\footnote{\url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/blob/main/rebMIGraph/rebmi_adapter.py\#L159-290}} implements advanced feature extraction to enhance attack performance:

\begin{itemize}
\item \textbf{Degree Features:} In/out/total degrees plus normalized variants capture connectivity patterns that differ between training and test nodes due to overfitting
\item \textbf{Local Structure:} Clustering coefficients and triangle counts quantify neighborhood density, exploiting memorized local patterns
\item \textbf{Ego-network:} Average neighbor degrees reveal second-order connectivity patterns the model memorizes during training
\item \textbf{Structural Roles:} Hub/leaf/isolated node indicators exploit position-specific model behaviors
\item \textbf{Feature-Graph Interaction:} Degree-weighted features and connectivity-scaled variance amplify membership signals
\item \textbf{Distance Metrics:} Proximity to high-degree nodes provides positional information that distinguishes members from non-members
\end{itemize}

These features transform raw attributes into a rich representation where structural and behavioral patterns expose membership through model overfitting characteristics.

\subsection{Attack Model Enhancement}
The improved attack model architecture\footnote{Attack model in TSTS.py: \url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/blob/main/rebMIGraph/TSTS.py\#L1725-1811}} includes several improvements:

\begin{itemize}
\item \textbf{Multi-layer Perceptron:} Three hidden layers with [128, 64, 32] neurons
\item \textbf{Regularization:} Dropout (p=0.3) and L2 weight decay ($\lambda=0.001$)
\item \textbf{Ensemble Methods:} Combines predictions from multiple shadow models
\item \textbf{Confidence Calibration:} Temperature scaling for posterior probability adjustment
\end{itemize}

\section{Results and Experimentation}
\label{results}
Extensive experiments were conducted to evaluate attack performance across different GNN architectures, datasets, and settings. Each experiment runs multiple times with different random seeds to ensure statistical significance.

\subsection{Experimental Setup}
\textbf{Hardware:} MacBook Air M3 with 16GB system memory\\
\textbf{Software:} PyTorch 1.13, PyTorch Geometric 2.2, Python 3.11\\
\textbf{Training:} 300 epochs for target/shadow models, no early stopping to maximize overfitting\\
\textbf{Metrics:} Attack accuracy, AUROC, precision, recall, F1-score\\
\textbf{Code:} Full experimental pipeline at \url{https://gitlab.rhrk.uni-kl.de/div26fuz/uni-project-dec/-/tree/main/rebMIGraph}

\subsection{Attack Performance}
Results show different vulnerability levels across GNN architectures:

\textbf{GCN Performance:} Showed strong resistance with 49.5\% $\pm$ 0.4\% attack accuracy on Twitch dataset and 49.7\% $\pm$ 0.3\% on Event dataset. The near-random performance shows effective protection against membership leakage.

\textbf{GAT Performance:} Showed moderate vulnerability with 49.1\% $\pm$ 0.8\% accuracy on Twitch and 51.7\% $\pm$ 3.8\% on Event. High variance on Event dataset (45.4\%-54.6\%) suggests dataset-dependent attack patterns.

\textbf{GraphSAGE Performance:} Showed consistent moderate vulnerability with 50.1\% $\pm$ 0.5\% accuracy on Twitch and 50.9\% $\pm$ 0.4\% on Event, with neighborhood sampling creating patterns that can be exploited.

\textbf{SGC Performance:} Most vulnerable architecture with 52.2\% $\pm$ 0.4\% accuracy on Twitch and very high 64.9\% $\pm$ 1.8\% on Event dataset, showing significant privacy risks due to its simple architecture.

\subsection{Synthetic Data Effectiveness}
Comparing synthetic and real shadow training data shows important insights:

\textbf{Baseline Comparisons:} Traditional datasets (Cora: 70.5\%, CiteSeer: 83.4\%) show much higher vulnerability than our custom datasets, validating our experimental setup.

\textbf{Synthetic vs Real Shadow Data:}
\begin{itemize}
\item Event dataset: 61.4\% (real) vs 49.7\% (synthetic) for GCN
\item Twitch dataset: 48.5\% (real) vs 49.5\% (synthetic) for GCN
\item GAT Event: 60.7\% (real) vs 51.7\% (synthetic)
\item GAT Twitch: 49.4\% (real) vs 49.1\% (synthetic)
\end{itemize}

\textbf{Key Finding:} synthetic data reduces attack effectiveness compared to real shadow training data, achieving about 60-70\% of baseline performance while still allowing viable attacks.

\section{Evaluation and Analysis}
\subsection{Statistical Analysis}
Results show clear vulnerability patterns that depend on architecture:

\textbf{Vulnerability Ranking:}
\begin{enumerate}
\item \textbf{SGC (Highest Risk):} 52.2\%-64.9\% attack accuracy
\item \textbf{GraphSAGE (Moderate):} 50.1\%-50.9\% attack accuracy  
\item \textbf{GAT (Variable):} 49.1\%-51.7\% with high variance
\item \textbf{GCN (Most Resistant):} 49.5\%-49.7\% near-random performance
\end{enumerate}

\textbf{Dataset Impact:} Event dataset shows consistently higher vulnerability across all architectures, with SGC achieving 64.9\% attack accuracy compared to 52.2\% on Twitch.

\textbf{Member vs Non-Member Classification:} Extreme patterns observed with some runs achieving 97\%+ accuracy on members but <5\% on non-members, indicating model overfitting creates distinguishable patterns.

\subsection{Performance Variance Analysis}
\textbf{Consistency Rankings:}
\begin{itemize}
\item \textbf{Most Consistent:} GCN ($\sigma$ < 0.4\%) and SGC on Twitch
\item \textbf{High Variance:} GAT on Event dataset ($\sigma$ = 3.8\%)
\item \textbf{Architecture Dependency:} Attention mechanisms (GAT) show dataset-sensitive performance
\end{itemize}

\section{Discussion}
The results show that synthetic graph data can effectively enable membership inference attacks without direct access to real training data patterns. This finding has important implications for privacy protection in graph learning systems.

\subsection{Privacy Implications}
The experimental results show important insights for GNN deployment in privacy-sensitive contexts:

\textbf{Architecture Selection Impact:} The choice of GNN architecture greatly affects privacy risk, with SGC showing 3x higher vulnerability than GCN on certain datasets.

\textbf{Synthetic Data Threat:} While synthetic shadow training data reduces attack effectiveness, it still enables viable membership inference attacks without requiring access to real training data patterns.

\textbf{Dataset Characteristics:} Demographic features (Event dataset) appear more exploitable than behavioral features (Twitch dataset), suggesting privacy risks vary by application area.


\subsection{Limitations}
Several constraints limit the scope and generalizability of findings:

\textbf{Experimental Limitations:}
\begin{itemize}
\item \textbf{Dataset Scope:} Limited to two social network datasets; broader area evaluation needed
\item \textbf{Synthetic Data Quality:} The quality of synthetic data provided by DLGrapher was not directly measured or compared to other generation methods
\item \textbf{Subgraph Approach:} Fixed 120-node subgraphs may not represent full-graph scenarios
\item \textbf{Architecture Parameters:} Fixed hyperparameters may not be best across all models
\end{itemize}

\textbf{Methodological Constraints:}
\begin{itemize}
\item \textbf{Attack Sophistication:} Basic attack model; advanced methods might show different results
\item \textbf{Defense Evaluation:} No comparison with state-of-the-art privacy-preserving methods
\item \textbf{Scalability:} Computer cost limits extensive hyperparameter exploration
\end{itemize}

\section{Conclusion}
This project successfully demonstrates membership inference attacks on Graph Neural Networks using synthetic data, revealing moderate attack success rates (49\%-65\% across architectures and datasets) while removing the attacker's need for real training data access. Adapting existing MIA frameworks to work with synthetic graphs provided by the DLGrapher project opens new attack vectors that organizations must consider when deploying GNN systems.

Key contributions include developing the TSTS methodology for subgraph-based attacks, comprehensive evaluation across four GNN architectures (GCN, GAT, GraphSAGE, SGC), and experimentation on real-world social network datasets. The experimental results show a clear vulnerability ranking: SGC shows highest risk (52.2\%-64.9\% attack accuracy), followed by GraphSAGE (50.1\%-50.9\%) and GAT (49.1\%-51.7\%), while GCN shows strong resistance with near-random performance (49.5\%-49.7\%).

The findings show important privacy implications for GNN deployment. While synthetic data-based attacks are less effective than traditional approaches, they show that attackers can infer membership without access to real training data. The results stress the critical importance of architecture selection in privacy-sensitive applications, with GCN providing much better privacy protection than SGC (49.7\% vs 64.9\% attack success on Event dataset). As graph learning continues to expand into critical areas involving personal data, these architectural considerations become very important for maintaining user privacy and regulatory compliance.

\section{Future Work}
Several promising directions extend this research:

\textbf{Advanced Synthetic Generation:} Exploring other graph generation methods beyond those provided by DLGrapher, including GANs and VAEs specifically designed for graph structures.

\textbf{Adaptive Attacks:} Developing attacks that dynamically adjust to target model defenses through reinforcement learning or adversarial training.

\textbf{Cross-Domain Evaluation:} Extending evaluation to biological networks, knowledge graphs, and recommendation systems to test generalizability.

\textbf{Defense Development:} Developing GNN-specific defense mechanisms that maintain utility while provably limiting membership leakage.

\textbf{Theoretical Analysis:} Establishing formal privacy guarantees and determining theoretical limits on membership inference success rates.

\textbf{Federated Learning:} Investigating membership inference in federated GNN settings where data remains distributed across multiple parties.



\bibliography{citations}
\bibliographystyle{unsrt}

\end{document}